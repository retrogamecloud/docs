---
title: 4.6 Procedimientos de Recuperación Ante Desastres
description: Guía completa de disaster recovery con procedimientos paso a paso, RPO/RTO definidos por servicio, runbooks de recuperación detallados y programa de simulacros trimestrales
icon: shield-halved
---

# 4.6 Procedimientos de Recuperación ante Desastres

Esta guía establece los procedimientos completos de recuperación ante desastres (DR) para la plataforma RetroGameCloud, incluyendo objetivos de tiempo y punto de recuperación específicos por servicio, runbooks detallados de recuperación y programa de simulacros.

## 4.6.1 Objetivos de Recuperación Detallados

### Definiciones Clave

<Card title="RPO - Recovery Point Objective" icon="clock">
  **Variable por servicio** - Pérdida máxima de datos aceptable según criticidad
</Card>

<Card title="RTO - Recovery Time Objective" icon="stopwatch">
  **Variable por servicio** - Tiempo máximo para restaurar servicios según prioridad
</Card>

<Card title="MTTR - Mean Time To Recovery" icon="wrench">
  **Tiempo promedio real** - Métrica histórica de recuperación
</Card>

### Matriz de Criticidad y Objetivos por Servicio

| Servicio | RPO | RTO | MTTR* | Prioridad | Dependencias | Notas |
|----------|-----|-----|-------|-----------|--------------|-------|
| **PostgreSQL RDS** | 5 min | 15 min | 12 min | P0-Crítica | Ninguna | Multi-AZ + Read Replicas |
| **EKS Control Plane** | 10 min | 20 min | 18 min | P0-Crítica | VPC, IAM | Cluster distribuido |
| **API Gateway (Kong)** | 15 min | 25 min | 20 min | P0-Crítica | EKS, RDS | Load balancer activo |
| **Redis ElastiCache** | 30 min | 10 min | 8 min | P1-Alta | Ninguna | Cluster mode enabled |
| **Core Applications** | 15 min | 45 min | 35 min | P1-Alta | EKS, RDS, Redis | Microservicios críticos |
| **Frontend/CDN** | 1 hora | 30 min | 25 min | P1-Alta | S3, CloudFront | Assets estáticos |
| **Monitoring Stack** | 30 min | 1 hora | 45 min | P2-Media | EKS | Prometheus, Grafana |
| **Logging (ELK)** | 2 horas | 1.5 horas | 1 hora | P2-Media | EKS | Elasticsearch cluster |
| **CI/CD Pipeline** | 4 horas | 2 horas | 90 min | P3-Baja | EKS | Jenkins/ArgoCD |
| **Backup Systems** | 24 horas | 4 horas | 3 horas | P3-Baja | S3 | Velero, snapshots |

<Info>*MTTR basado en incidentes de los últimos 12 meses</Info>

## 4.6.2 Runbooks de Recuperación por Servicio

### 4.6.2.1 PostgreSQL RDS - Runbook de Recuperación

<Tabs>
<Tab title="Escenario: Falla de Instancia Principal">

#### Detección Automática
```bash
# Verificar estado RDS
aws rds describe-db-instances \
  --db-instance-identifier retrogame-prod-primary \
  --query 'DBInstances[0].DBInstanceStatus'

# Monitorear métricas CloudWatch
aws cloudwatch get-metric-statistics \
  --namespace AWS/RDS \
  --metric-name DatabaseConnections \
  --dimensions Name=DBInstanceIdentifier,Value=retrogame-prod-primary
```

#### Procedimiento de Failover
```bash
#!/bin/bash
# recovery-rds-failover.sh

echo "=== INICIANDO FAILOVER RDS ==="
echo "Timestamp: $(date)"

# 1. Verificar Multi-AZ status
aws rds describe-db-instances \
  --db-instance-identifier retrogame-prod-primary \
  --query 'DBInstances[0].MultiAZ'

# 2. Ejecutar failover automático
aws rds reboot-db-instance \
  --db-instance-identifier retrogame-prod-primary \
  --force-failover

# 3. Monitorear progreso
while true; do
  STATUS=$(aws rds describe-db-instances \
    --db-instance-identifier retrogame-prod-primary \
    --query 'DBInstances[0].DBInstanceStatus' \
    --output text)
  
  echo "Estado actual: $STATUS"
  
  if [ "$STATUS" = "available" ]; then
    echo "✅ Failover completado exitosamente"
    break
  fi
  
  sleep 30
done

# 4. Verificar conectividad
psql -h retrogame-prod-primary.cluster-xyz.us-east-1.rds.amazonaws.com \
     -U admin -d retrogame -c "SELECT 1;"

echo "=== FAILOVER RDS COMPLETADO ==="
```

#### Verificaciones Post-Recuperación
- [ ] Conectividad desde aplicaciones
- [ ] Replicación funcionando
- [ ] Métricas de performance normales
- [ ] Logs sin errores críticos

</Tab>

<Tab title="Escenario: Corrupción de Datos">

#### Procedimiento de Point-in-Time Recovery
```bash
#!/bin/bash
# recovery-rds-pitr.sh

RESTORE_TIME="2024-01-15T10:30:00.000Z"
NEW_INSTANCE_ID="retrogame-prod-restored-$(date +%Y%m%d-%H%M)"

echo "=== INICIANDO POINT-IN-TIME RECOVERY ==="

# 1. Crear nueva instancia desde backup
aws rds restore-db-instance-to-point-in-time \
  --source-db-instance-identifier retrogame-prod-primary \
  --target-db-instance-identifier $NEW_INSTANCE_ID \
  --restore-time $RESTORE_TIME \
  --db-subnet-group-name retrogame-db-subnet-group \
  --vpc-security-group-ids sg-1234567890abcdef0

# 2. Esperar disponibilidad
aws rds wait db-instance-available \
  --db-instance-identifier $NEW_INSTANCE_ID

# 3. Obtener nuevo endpoint
NEW_ENDPOINT=$(aws rds describe-db-instances \
  --db-instance-identifier $NEW_INSTANCE_ID \
  --query 'DBInstances[0].Endpoint.Address' \
  --output text)

echo "Nueva instancia disponible en: $NEW_ENDPOINT"

# 4. Validar datos
psql -h $NEW_ENDPOINT -U admin -d retrogame << EOF
SELECT COUNT(*) FROM users;
SELECT COUNT(*) FROM games;
SELECT MAX(created_at) FROM user_sessions;
EOF

echo "=== POINT-IN-TIME RECOVERY COMPLETADO ==="
```

</Tab>

<Tab title="Escenario: Disaster Recovery Cross-Region">

#### Procedimiento de Recuperación Regional
```bash
#!/bin/bash
# recovery-rds-cross-region.sh

DR_REGION="us-west-2"
SNAPSHOT_ID="retrogame-prod-manual-$(date +%Y%m%d)"

echo "=== INICIANDO DR CROSS-REGION ==="

# 1. Crear snapshot manual
aws rds create-db-snapshot \
  --db-snapshot-identifier $SNAPSHOT_ID \
  --db-instance-identifier retrogame-prod-primary

# 2. Copiar snapshot a región DR
aws rds copy-db-snapshot \
  --source-db-snapshot-identifier $SNAPSHOT_ID \
  --target-db-snapshot-identifier $SNAPSHOT_ID-dr \
  --source-region us-east-1 \
  --target-region $DR_REGION

# 3. Restaurar en región DR
aws rds restore-db-instance-from-db-snapshot \
  --db-instance-identifier retrogame-prod-dr \
  --db-snapshot-identifier $SNAPSHOT_ID-dr \
  --region $DR_REGION

echo "=== DR CROSS-REGION EN PROGRESO ==="
```

</Tab>
</Tabs>

### 4.6.2.2 EKS Cluster - Runbook de Recuperación

<Tabs>
<Tab title="Escenario: Falla del Control Plane">

#### Diagnóstico y Detección
```bash
#!/bin/bash
# diagnosis-eks-control-plane.sh

echo "=== DIAGNÓSTICO EKS CONTROL PLANE ==="

# 1. Verificar estado del cluster
aws eks describe-cluster \
  --name retrogame-prod \
  --query 'cluster.status'

# 2. Verificar endpoints de API
kubectl cluster-info

# 3. Verificar nodos
kubectl get nodes -o wide

# 4. Verificar servicios críticos
kubectl get pods -n kube-system

# 5. Verificar eventos recientes
kubectl get events --sort-by='.lastTimestamp' | tail -20
```

#### Procedimiento de Recuperación
```bash
#!/bin/bash
# recovery-eks-control-plane.sh

echo "=== RECUPERACIÓN EKS CONTROL PLANE ==="

# 1. Verificar y actualizar kubeconfig
aws eks update-kubeconfig \
  --region us-east-1 \
  --name retrogame-prod

# 2. Verificar conectividad
kubectl get nodes --request-timeout=10s

# 3. Reiniciar servicios críticos si es necesario
kubectl rollout restart deployment/coredns -n kube-system

# 4. Verificar pods críticos
kubectl get pods -n retrogame-system -o wide

# 5. Restaurar aplicaciones desde backup
helm list -n retrogame-prod
argocd app sync retrogame-core-apps
```

#### Escalación Automática
```yaml
# eks-recovery-automation.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: recovery-scripts
  namespace: kube-system
data:
  auto-recovery.sh: |
    #!/bin/bash
    # Verificar cada 30 segundos
    while true; do
      if ! kubectl get nodes &>/dev/null; then
        echo "Control plane no disponible - ejecutando recuperación"
        /scripts/recovery-eks-control-plane.sh
        sleep 300  # Esperar 5 minutos antes del siguiente intento
      fi
      sleep 30
    done
```

</Tab>

<Tab title="Escenario: Pérdida de Nodos Worker">

#### Diagnóstico de Nodos
```bash
#!/bin/bash
# diagnosis-eks-nodes.sh

echo "=== DIAGNÓSTICO NODOS WORKER ==="

# 1. Estado de nodos
kubectl get nodes -o custom-columns=NAME:.metadata.name,STATUS:.status.conditions[-1].type,READY:.status.conditions[-1].status

# 2. Capacidad y utilización
kubectl top nodes

# 3. Pods sin programar
kubectl get pods --field-selector=status.phase=Pending -A

# 4. Verificar Auto Scaling Groups
aws autoscaling describe-auto-scaling-groups \
  --auto-scaling-group-names retrogame-prod-nodes
```

#### Recuperación de Nodos
```bash
#!/bin/bash
# recovery-eks-nodes.sh

ASG_NAME="retrogame-prod-nodes"
DESIRED_CAPACITY=6

echo "=== RECUPERACIÓN NODOS WORKER ==="

# 1. Escalar ASG si es necesario
aws autoscaling update-auto-scaling-group \
  --auto-scaling-group-name $ASG_NAME \
  --desired-capacity $DESIRED_CAPACITY

# 2. Verificar instancias EC2
aws ec2 describe-instances \
  --filters "Name=tag:aws:autoscaling:groupName,Values=$ASG_NAME" \
  --query 'Reservations[].Instances[].[InstanceId,State.Name]'

# 3. Verificar registro de nodos
kubectl get nodes -w &
WATCH_PID=$!

# Esperar hasta que los nodos estén listos
sleep 300
kill $WATCH_PID

# 4. Redistribuir pods si es necesario
kubectl get pods -o wide -A | grep Pending | while read line; do
  NAMESPACE=$(echo $line | awk '{print $1}')
  POD=$(echo $line | awk '{print $2}')
  kubectl delete pod $POD -n $NAMESPACE
done
```

#### Configuración Cluster Autoscaler
```yaml
# cluster-autoscaler-config.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  template:
    spec:
      containers:
      - name: cluster-autoscaler
        image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.24.0
        command:
        - ./cluster-autoscaler
        - --v=4
        - --stderrthreshold=info
        - --cloud-provider=aws
        - --skip-nodes-with-local-storage=false
        - --expander=least-waste
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/retrogame-prod
        - --balance-similar-node-groups
        - --scale-down-delay-after-add=10m
        - --scale-down-unneeded-time=10m
```

</Tab>

<Tab title="Escenario: Pérdida de Datos de Aplicaciones">

#### Recuperación con Velero
```bash
#!/bin/bash
# recovery-eks-velero.sh

BACKUP_NAME="daily-backup-$(date -d '1 day ago' +%Y%m%d)"

echo "=== RECUPERACIÓN CON VELERO ==="

# 1. Listar backups disponibles
velero backup get

# 2. Verificar backup específico
velero backup describe $BACKUP_NAME

# 3. Crear restore desde backup
velero restore create restore-$(date +%Y%m%d-%H%M) \
  --from-backup $BACKUP_NAME \
  --namespace-mappings retrogame-prod:retrogame-prod

# 4. Monitorear progreso
velero restore get
velero restore logs restore-$(date +%Y%m%d-%H%M)

# 5. Verificar aplicaciones
kubectl get pods -n retrogame-prod
kubectl get pvc -n retrogame-prod
```

#### Recuperación Selectiva
```bash
#!/bin/bash
# selective-restore.sh

echo "=== RECUPERACIÓN SELECTIVA ==="

# Recuperar solo un namespace específico
velero restore create restore-api-$(date +%Y%m%d) \
  --from-backup $BACKUP_NAME \
  --include-namespaces retrogame-api

# Recuperar solo recursos específicos
velero restore create restore-db-$(date +%Y%m%d) \
  --from-backup $BACKUP_NAME \
  --include-resources persistentvolumes,persistentvolumeclaims

# Excluir recursos problemáticos
velero restore create restore-safe-$(date +%Y%m%d) \
  --from-backup $BACKUP_NAME \
  --exclude-resources nodes,events
```

</Tab>
</Tabs>

### 4.6.2.3 Redis ElastiCache - Runbook de Recuperación

<Tabs>
<Tab title="Esc
</Tabs>