---
title: 8.5. Runbooks Operacionales
description: Procedimientos detallados para respuesta a incidentes críticos en producción
icon: life-ring
---

# 8.5. Runbooks Operacionales

Esta sección proporciona procedimientos paso a paso para responder a incidentes críticos en el entorno de producción de RetroGameCloud.

<Note>
Estos runbooks están diseñados para ser ejecutados por el equipo de operaciones con acceso a los sistemas de producción. Mantén estos procedimientos actualizados y practica regularmente.
</Note>

## 8.5.1. Caída de Base de Datos

### Síntomas

- Errores de conexión a base de datos en logs de microservicios

- Timeouts en queries

- Alertas de Prometheus: `postgres_up == 0`

- Dashboard de Grafana mostrando 0 conexiones activas

### Procedimiento de Respuesta

<Tabs>
<Tab title="Diagnóstico Inicial">

```bash

# 1. Verificar estado del cluster PostgreSQL
kubectl get pods -n database -l app=postgresql
kubectl describe pod postgresql-primary-0 -n database

# 2. Verificar logs de PostgreSQL
kubectl logs postgresql-primary-0 -n database --tail=100

# 3. Comprobar recursos del nodo
kubectl top nodes
kubectl describe node <node-name>

# 4. Verificar almacenamiento persistente
kubectl get pv,pvc -n database

```

</Tab>
<Tab title="Recuperación Automática">

```bash

# 1. Reinicio del pod principal
kubectl delete pod postgresql-primary-0 -n database

# 2. Esperar a que el pod se recupere
kubectl wait --for=condition=Ready pod/postgresql-primary-0 -n database --timeout=300s

# 3. Verificar conectividad desde microservicios
kubectl exec -it deployment/auth-service -n retrogame -- \
  pg_isready -h postgresql.database.svc.cluster.local -p 5432

# 4. Ejecutar health check básico
kubectl exec -it postgresql-primary-0 -n database -- \
  psql -U postgres -c "SELECT version();"

```

</Tab>
<Tab title="Recuperación Manual">

```bash

# Si el reinicio automático falla

# 1. Verificar backup más reciente
aws s3 ls s3://retrogame-backups/postgresql/ --recursive | tail -5

# 2. Crear nuevo volumen desde snapshot (si es necesario)
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgresql-data-recovery
  namespace: database
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
  storageClassName: gp2
EOF

# 3. Restaurar desde backup
kubectl create job postgresql-restore --image=postgres:13 -- \
  /bin/bash -c "
  aws s3 cp s3://retrogame-backups/postgresql/latest.sql /tmp/restore.sql
  psql -h postgresql.database.svc.cluster.local -U postgres < /tmp/restore.sql
  "

# 4. Verificar integridad de datos
kubectl exec -it postgresql-primary-0 -n database -- \
  psql -U postgres -d retrogame -c "
  SELECT schemaname, tablename, n_tup_ins, n_tup_upd
  FROM pg_stat_user_tables
  ORDER BY n_tup_ins DESC LIMIT 10;
  "

```

</Tab>
</Tabs>

### Escalación

- **Nivel 1** (0-15 min): Reinicio automático

- **Nivel 2** (15-30 min): Contactar DBA senior

- **Nivel 3** (30+ min): Activar equipo de emergencia y considerar failover

## 8.5.2. Saturación de Redis

### Síntomas

- Latencia alta en autenticación y sesiones

- Errores `OOM` en logs de Redis

- Métricas de memoria Redis > 90%

- Cache miss ratio > 80%

### Procedimiento de Respuesta

<Tabs>
<Tab title="Análisis de Memoria">

```bash

# 1. Verificar uso de memoria actual
kubectl exec -it redis-master-0 -n cache -- redis-cli INFO memory

# 2. Identificar keys más grandes
kubectl exec -it redis-master-0 -n cache -- redis-cli --bigkeys

# 3. Verificar distribución de tipos de datos
kubectl exec -it redis-master-0 -n cache -- redis-cli INFO keyspace

# 4. Comprobar configuración de eviction
kubectl exec -it redis-master-0 -n cache -- redis-cli CONFIG GET maxmemory-policy

```

</Tab>
<Tab title="Limpieza de Emergencia">

```bash

# 1. Eliminar keys expiradas manualmente
kubectl exec -it redis-master-0 -n cache -- redis-cli FLUSHDB 1

# 2. Limpiar sessions antiguas (más de 24h)
kubectl exec -it redis-master-0 -n cache -- redis-cli --eval - 0 <<EOF
local keys = redis.call('KEYS', 'session:*')
local deleted = 0
for i=1,#keys do
  local ttl = redis.call('TTL', keys[i])
  if ttl < 0 or ttl > 86400 then
    redis.call('DEL', keys[i])
    deleted = deleted + 1
  end
end
return deleted
EOF

# 3. Configurar política de eviction más agresiva
kubectl exec -it redis-master-0 -n cache -- \
  redis-cli CONFIG SET maxmemory-policy allkeys-lru

# 4. Reducir TTL de cache de juegos temporalmente
kubectl exec -it redis-master-0 -n cache -- \
  redis-cli CONFIG SET timeout 300

```

</Tab>
<Tab title="Escalado Horizontal">

```bash

# 1. Añadir replica adicional
kubectl patch statefulset redis-replica -n cache -p '{"spec":{"replicas":3}}'

# 2. Reconfigurar balanceador para distribuir lecturas
kubectl apply -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  name: redis-read
  namespace: cache
spec:
  selector:
    app: redis
    role: replica
  ports:
  - port: 6379
    targetPort: 6379
  sessionAffinity: None
EOF

# 3. Actualizar configuración de microservicios
kubectl set env deployment/auth-service -n retrogame \
  REDIS_READ_HOST=redis-read.cache.svc.cluster.local

# 4. Verificar distribución de carga
kubectl exec -it redis-replica-0 -n cache -- redis-cli INFO replication

```

</Tab>
</Tabs>

## 8.5.3. Problemas de Red y Conectividad

### Síntomas

- Timeouts entre microservicios

- Latencia alta en API Gateway

- Pérdida de paquetes en métricas de red

- Errores de DNS resolution

### Procedimiento de Respuesta

<Tabs>
<Tab title="Diagnóstico de Red">

```bash

# 1. Verificar conectividad entre namespaces
kubectl run netshoot --rm -i --tty --image nicolaka/netshoot -- /bin/bash

# Dentro del pod:
nslookup auth-service.retrogame.svc.cluster.local
ping auth-service.retrogame.svc.cluster.local
curl -v http://auth-service.retrogame.svc.cluster.local:8080/health

# 2. Verificar políticas de red
kubectl get networkpolicies --all-namespaces

# 3. Comprobar estado de CNI
kubectl get pods -n kube-system -l k8s-app=aws-node
kubectl logs -n kube-system -l k8s-app=aws-node --tail=50

# 4. Verificar DNS del cluster
kubectl get pods -n kube-system -l k8s-app=kube-dns
kubectl exec -it deployment/coredns -n kube-system -- nslookup kubernetes.default

```

</Tab>
<Tab title="Resolución DNS">

```bash

# 1. Reiniciar CoreDNS
kubectl rollout restart deployment/coredns -n kube-system

# 2. Limpiar cache DNS de pods
kubectl delete pods -n retrogame -l app=auth-service
kubectl delete pods -n retrogame -l app=user-service
kubectl delete pods -n retrogame -l app=game-catalog-service

# 3. Verificar configuración DNS
kubectl get configmap coredns -n kube-system -o yaml

# 4. Test de resolución desde cada microservicio
for service in auth-service user-service game-catalog-service; do
  kubectl exec -it deployment/$service -n retrogame -- \
    nslookup postgresql.database.svc.cluster.local
done

```

</Tab>
<Tab title="Problemas de CNI">

```bash

# 1. Verificar IPs disponibles en subnets
aws ec2 describe-subnets --subnet-ids subnet-xxx --query 'Subnets[0].AvailableIpAddressCount'

# 2. Reiniciar CNI en nodos problemáticos
kubectl get nodes -o wide
kubectl cordon <node-name>
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data

# 3. Verificar security groups
aws ec2 describe-security-groups --group-ids sg-xxx

# 4. Comprobar rutas de VPC
aws ec2 describe-route-tables --route-table-ids rtb-xxx

# 5. Reiniciar nodo si es necesario
aws ec2 reboot-instances --instance-ids i-xxx
kubectl uncordon <node-name>

```

</Tab>
</Tabs>

## 8.5.4. Rollback de Despliegues

### Síntomas

- Aumento significativo en error rate después de despliegue

- Funcionalidad crítica no disponible

- Performance degradada en nuevas versiones

### Procedimiento de Rollback

<Tabs>
<Tab title="Rollback Automático">

```bash

# 1. Verificar historial de deployments
kubectl rollout history deployment/auth-service -n retrogame
kubectl rollout history deployment/user-service -n retrogame
kubectl rollout history deployment/game-catalog-service -n retrogame

# 2. Rollback a versión anterior
kubectl rollout undo deployment/auth-service -n retrogame
kubectl rollout undo deployment/user-service -n retrogame
kubectl rollout undo deployment/game-catalog-service -n retrogame

# 3. Verificar estado del rollback
kubectl rollout status deployment/auth-service -n retrogame --timeout=300s
kubectl rollout status deployment/user-service -n retrogame --timeout=300s

# 4. Verificar health checks
curl -f http://api.retrogame.local/health
curl -f http://api.retrogame.local/auth/health

```

</Tab>
<Tab title="Rollback GitOps">

```bash

# 1. Identificar commit problemático en ArgoCD
kubectl get applications -n argocd
argocd app get retrogame-app --show-params

# 2. Revertir cambios en Git
git log --oneline -10
git revert <commit-hash> --no-edit
git push origin main

# 3. Forzar sync en ArgoCD
argocd app sync retrogame-app --prune
argocd app wait retrogame-app --timeout 300

# 4. Verificar sincronización
argocd app get retrogame-app
kubectl get pods -n retrogame -o wide

```

</Tab>
<Tab title="Rollback de Base de Datos">

```bash

# 1. Verificar migraciones aplicadas
kubectl exec -it postgresql-primary-0 -n database -- \
  psql -U postgres -d retrogame -c "SELECT * FROM schema_migrations ORDER BY version DESC LIMIT 10;"

# 2. Crear backup antes del rollback
kubectl exec -it postgresql-primary-0 -n database -- \
  pg_dump -U postgres retrogame > /tmp/pre-rollback-backup.sql

# 3. Ejecutar rollback de migraciones (si aplicable)
kubectl create job db-rollback --image=migrate/migrate -- \
  -path=/migrations -database="postgres://postgres:password@postgresql.database.svc.cluster.local/retrogame?sslmode=disable" \
  down 1

# 4. Verificar integridad post-rollback
kubectl exec -it postgresql-primary-0 -n database -- \
  psql -U postgres -d retrogame -c "
  SELECT table_name, column_name, data_type
  FROM information_schema.columns
  WHERE table_schema = 'public'
  ORDER BY table_name, ordinal_position;
  "

```

</Tab>
</Tabs>

## 8.5.5. Escalado de Emergencia

### Síntomas

- CPU/Memoria > 80% mantenida por más de 5 minutos

- Response time > 2 segundos

- Queue length creciente en API Gateway

- Alertas de saturación de recursos

### Procedimiento de Escalado

<Tabs>
<Tab title="Escalado de Microservicios">

```bash

# 1. Identificar servicios con alta carga
kubectl top pods -n retrogame --sort-by=cpu
kubectl top pods -n retrogame --sort-by=memory

# 2. Escalado inmediato manual
kubectl scale deployment auth-service -n retrogame --replicas=6
kubectl scale deployment user-service -n retrogame --replicas=4
kubectl scale deployment game-catalog-service -n retrogame --replicas=8

# 3. Verificar distribución de pods
kubectl get pods -n retrogame -o wide

# 4. Monitorear métricas post-escalado
watch "kubectl top pods -n retrogame"

```

</Tab>
<Tab title="Escalado de Cluster">

```bash

# 1. Verificar capacidad actual del cluster
kubectl describe nodes | grep -A 5 "Allocated resources"
kubectl get nodes -o custom-columns=NAME:.metadata.name,CPU:.status.capacity.cpu,MEMORY:.status.capacity.memory

# 2. Escalar node groups en EKS
aws eks describe-nodegroup --cluster-name retrogame-cluster --nodegroup-name workers
aws eks update-nodegroup-config --cluster-name retrogame-cluster --nodegroup-name workers --scaling-config minSize=3,maxSize=12,desiredSize=8

# 3. Verificar nuevos nodos
kubectl get nodes -w

# 4. Configurar cluster autoscaler si no está activo
kubectl patch deployment cluster-autoscaler -n kube-system -p '{"spec":{"template":{"spec":{"containers":[{"name":"cluster-autoscaler","resources":{"requests":{"cpu":"100m","memory":"300Mi"}}}]}}}}'

```

</Tab>
<Tab title="Escalado de Base de Datos">

```bash

# 1. Verificar métricas de PostgreSQL
kubectl exec -it postgresql-primary-0 -n database -- \
  psql -U postgres -c "
  SELECT datname, numbackends, xact_commit, xact_rollback, blks_read, blks_hit
  FROM pg_stat_database
  WHERE datname = 'retrogame';
  "

# 2. Añadir replicas de lectura
kubectl patch postgresql postgresql-cluster -n database --type='merge' -p='{"spec":{"instances":3}}'

# 3. Configurar connection pooling
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
</Tab>
</Tabs>

```