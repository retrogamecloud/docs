---
title: 4.5. Runbooks Operacionales
description: Procedimientos detallados para respuesta a incidentes críticos en producción
icon: life-ring
---

# 4.5. Runbooks Operacionales

Esta sección proporciona procedimientos paso a paso para responder a incidentes críticos en el entorno de producción de RetroGameCloud.

<Note>
Estos runbooks están diseñados para ser ejecutados por el equipo de operaciones con acceso a los sistemas de producción. Mantén estos procedimientos actualizados y practica regularmente.
</Note>

## 4.5.1. Caída de Base de Datos

### Síntomas

- Errores de conexión a base de datos en logs de microservicios
- Timeouts en queries
- Alertas de Prometheus: `postgres_up == 0`
- Dashboard de Grafana mostrando 0 conexiones activas

### Procedimiento de Respuesta

<Tabs>
<Tab title="Diagnóstico Inicial">

```bash
# 1. Verificar estado del cluster PostgreSQL
kubectl get pods -n database -l app=postgresql
kubectl describe pod postgresql-primary-0 -n database

# 2. Verificar logs de PostgreSQL
kubectl logs postgresql-primary-0 -n database --tail=100

# 3. Comprobar recursos del nodo
kubectl top nodes
kubectl describe node <node-name>

# 4. Verificar almacenamiento persistente
kubectl get pv,pvc -n database
```

</Tab>
<Tab title="Recuperación Automática">

```bash
# 1. Reinicio del pod principal
kubectl delete pod postgresql-primary-0 -n database

# 2. Esperar a que el pod se recupere
kubectl wait --for=condition=Ready pod/postgresql-primary-0 -n database --timeout=300s

# 3. Verificar conectividad desde microservicios
kubectl exec -it deployment/auth-service -n retrogame -- \
  pg_isready -h postgresql.database.svc.cluster.local -p 5432

# 4. Ejecutar health check básico
kubectl exec -it postgresql-primary-0 -n database -- \
  psql -U postgres -c "SELECT version();"
```

</Tab>
<Tab title="Recuperación Manual">

```bash
# Si el reinicio automático falla

# 1. Verificar backup más reciente
aws s3 ls s3://retrogame-backups/postgresql/ --recursive | tail -10

# 2. Restaurar desde backup (último punto válido)
kubectl create job postgresql-restore-$(date +%s) --from=cronjob/postgresql-backup -n database

# 3. Monitorizar restauración
kubectl logs job/postgresql-restore-$(date +%s) -n database -f

# 4. Verificar integridad después de restauración
kubectl exec -it postgresql-primary-0 -n database -- \
  psql -U postgres -c "SELECT COUNT(*) FROM users;"
```

</Tab>
</Tabs>

## 4.5.2. Fallos de Redis Cache

### Síntomas

- Latencia elevada en respuestas de API
- Errores de conexión a Redis en logs
- Alertas: `redis_up == 0`
- Incremento en queries directas a PostgreSQL

### Procedimiento de Respuesta

<Tabs>
<Tab title="Diagnóstico">

```bash
# 1. Verificar estado de Redis
kubectl get pods -n cache -l app=redis
kubectl logs redis-master-0 -n cache --tail=50

# 2. Verificar memoria y CPU
kubectl top pod redis-master-0 -n cache

# 3. Comprobar configuración
kubectl exec -it redis-master-0 -n cache -- redis-cli CONFIG GET maxmemory
```

</Tab>
<Tab title="Recuperación">

```bash
# 1. Reiniciar Redis master
kubectl delete pod redis-master-0 -n cache

# 2. Verificar cluster Redis
kubectl exec -it redis-master-0 -n cache -- redis-cli CLUSTER NODES

# 3. Limpiar cache si es necesario
kubectl exec -it redis-master-0 -n cache -- redis-cli FLUSHALL

# 4. Verificar desde microservicio
kubectl exec -it deployment/game-service -n retrogame -- \
  redis-cli -h redis.cache.svc.cluster.local ping
```

</Tab>
</Tabs>

## 4.5.3. Sobrecarga de Tráfico

### Síntomas

- Alto número de requests/segundo en métricas
- Latencia P95 > 2s en APIs críticas
- Pods con CPU > 80%
- Alertas de HPA activándose constantemente

### Procedimiento de Respuesta

<Tabs>
<Tab title="Mitigación Inmediata">

```bash
# 1. Escalar servicios críticos manualmente
kubectl scale deployment auth-service --replicas=10 -n retrogame
kubectl scale deployment game-service --replicas=8 -n retrogame
kubectl scale deployment user-service --replicas=6 -n retrogame

# 2. Verificar recursos disponibles
kubectl describe nodes | grep -A 5 "Allocated resources"

# 3. Activar rate limiting agresivo
kubectl patch configmap nginx-config -n ingress-nginx \
  --patch '{"data":{"rate-limit":"10r/s"}}'
```

</Tab>
<Tab title="Análisis de Tráfico">

```bash
# 1. Identificar fuentes de tráfico
kubectl logs -n ingress-nginx deployment/nginx-ingress-controller | \
  awk '{print $1}' | sort | uniq -c | sort -nr | head -20

# 2. Verificar endpoints más utilizados
kubectl logs deployment/auth-service -n retrogame | \
  grep "GET\|POST" | awk '{print $7}' | sort | uniq -c | sort -nr

# 3. Comprobar métricas de Prometheus
curl -X GET 'http://prometheus:9090/api/v1/query?query=rate(http_requests_total[5m])'
```

</Tab>
<Tab title="Optimización">

```bash
# 1. Ajustar límites de recursos
kubectl patch deployment auth-service -n retrogame -p \
  '{"spec":{"template":{"spec":{"containers":[{"name":"auth-service","resources":{"limits":{"cpu":"2000m","memory":"1Gi"}}}]}}}}'

# 2. Configurar cache más agresivo
kubectl patch configmap redis-config -n cache \
  --patch '{"data":{"maxmemory":"2gb","maxmemory-policy":"allkeys-lru"}}'

# 3. Optimizar conexiones de BD
kubectl exec -it postgresql-primary-0 -n database -- \
  psql -U postgres -c "ALTER SYSTEM SET max_connections = 200;"
```

</Tab>
</Tabs>

## 4.5.4. Fallos de Microservicios

### Síntomas

- Pods en estado CrashLoopBackOff
- Health checks fallando
- Errores 5xx en respuestas HTTP
- Logs de aplicación con excepciones

### Procedimiento de Respuesta

<Tabs>
<Tab title="Diagnóstico">

```bash
# 1. Identificar servicios afectados
kubectl get pods -n retrogame | grep -E "(Error|CrashLoopBackOff|Pending)"

# 2. Examinar logs del servicio
kubectl logs deployment/game-service -n retrogame --tail=100

# 3. Verificar eventos del cluster
kubectl get events -n retrogame --sort-by=.metadata.creationTimestamp

# 4. Comprobar health checks
kubectl describe deployment game-service -n retrogame
```

</Tab>
<Tab title="Recuperación">

```bash
# 1. Rolling restart del servicio
kubectl rollout restart deployment/game-service -n retrogame

# 2. Verificar el rollout
kubectl rollout status deployment/game-service -n retrogame

# 3. Si el restart falla, rollback a versión anterior
kubectl rollout undo deployment/game-service -n retrogame

# 4. Verificar conectividad entre servicios
kubectl exec -it deployment/auth-service -n retrogame -- \
  curl -f http://game-service:8080/health
```

</Tab>
</Tabs>

## 4.5.5. Problemas de Red y Conectividad

### Síntomas

- Timeouts entre microservicios
- DNS resolution failures
- Intermitent connectivity issues
- Network policies bloqueando tráfico

### Procedimiento de Respuesta

<Tabs>
<Tab title="Diagnóstico de Red">

```bash
# 1. Verificar DNS interno
kubectl exec -it deployment/auth-service -n retrogame -- \
  nslookup game-service.retrogame.svc.cluster.local

# 2. Testear conectividad entre pods
kubectl exec -it deployment/auth-service -n retrogame -- \
  nc -zv game-service.retrogame.svc.cluster.local 8080

# 3. Verificar network policies
kubectl get networkpolicies -n retrogame
kubectl describe networkpolicy allow-microservices -n retrogame

# 4. Comprobar estado de CNI
kubectl get pods -n kube-system | grep -E "(calico|flannel|weave)"
```

</Tab>
<Tab title="Resolución">

```bash
# 1. Reiniciar pods de DNS
kubectl delete pod -n kube-system -l k8s-app=kube-dns

# 2. Reiniciar CNI pods si es necesario
kubectl delete pod -n kube-system -l k8s-app=calico-node

# 3. Verificar y ajustar network policies
kubectl apply -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-internal
  namespace: retrogame
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector: {}
  egress:
  - to:
    - namespaceSelector: {}
EOF
```

</Tab>
</Tabs>

## 4.5.6. Problemas de Almacenamiento

### Síntomas

- Pods pendientes por falta de PVs
- Errores de I/O en logs
- PVCs en estado Pending
- Alertas de espacio en disco

### Procedimiento de Respuesta

<Tabs>
<Tab title="Diagnóstico de Storage">

```bash
# 1. Verificar PVs y PVCs
kubectl get pv,pvc --all-namespaces

# 2. Comprobar storage classes
kubectl get storageclass

# 3. Verificar espacio en nodos
kubectl exec -it <pod-name> -n <namespace> -- df -h

# 4. Examinar eventos de storage
kubectl get events --field-selector reason=FailedMount
```

</Tab>
<Tab title="Resolución">

```bash
# 1. Limpiar volúmenes no utilizados
kubectl delete pv <pv-name> # Solo si está en estado Available

# 2. Expandir PVC si es posible
kubectl patch pvc data-postgresql-primary-0 -n database \
  -p '{"spec":{"resources":{"requests":{"storage":"200Gi"}}}}'

# 3. Crear nuevo PV si es necesario
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: manual-pv-001
spec:
  capacity:
    storage: 100Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: fast-ssd
  local:
    path: /mnt/disks/vol1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - worker-node-1
EOF
```

</Tab>
</Tabs>

## 4.5.7. Contactos de Escalamiento

### Niveles de Escalamiento

<Card title="Nivel 1 - Equipo de Operaciones" icon="users">
- **DevOps Engineer**: ops@retrogamecloud.com
- **SRE**: sre@retrogamecloud.com
- **Slack**: #ops-alerts
- **Tiempo de respuesta**: 15 minutos
</Card>

<Card title="Nivel 2 - Liderazgo Técnico" icon="user-tie">
- **Tech Lead**: tech-lead@retrogamecloud.com
- **Architecture Lead**: architect@retrogamecloud.com
- **Slack**: #critical-incidents
- **Tiempo de respuesta**: 30 minutos
</Card>

<Card title="Nivel 3 - Management" icon="crown">
- **CTO**: cto@retrogamecloud.com
- **VP Engineering**: vpe@retrogamecloud.com
- **Escalamiento**: Solo para incidentes de severidad crítica
- **Tiempo de respuesta**: 1 hora
</Card>

## 4.5.8. Post-Mortem Template

Después de resolver un incidente crítico, documenta usando este template:

```markdown
# Post-Mortem: [Título del Incidente]

## Resumen
- **Fecha**: 
- **Duración**: 
- **Impacto**: 
- **Root Cause**: 

## Timeline
- **HH:MM** - Detección inicial
- **HH:MM** - Escalamiento
- **HH:MM** - Mitigación implementada
- **HH:MM** - Resolución completa

## Acciones Tomadas
1. 
2. 
3. 

## Lecciones Aprendidas
- 
- 

## Action Items
- [ ] 
- [ ] 
- [ ] 

## Prevencion
- 
- 
```

<Warning>
Recuerda actualizar estos runbooks regularmente y realizar drills para validar su efectividad. La práctica regular es clave para una respuesta efectiva a incidentes.
</Warning>