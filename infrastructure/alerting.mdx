---
title: Alertas y Monitorización
description: 'Sistema de alertas, umbrales críticos y respuesta a incidentes para RetroGameCloud'
icon: bell
---

# Alertas y Monitorización

Sistema completo de alertas para detectar y responder a incidentes en RetroGameCloud antes de que afecten a los usuarios.

## Filosofía de Alertas

<Note>
Seguimos el principio **"Alert on symptoms, not causes"**. Las alertas deben indicar impacto en usuarios, no problemas técnicos internos que no afectan el servicio.
</Note>

### Golden Signals

<CardGroup cols={2}>
  <Card title="Latency" icon="clock">
    Tiempo de respuesta de requests
  </Card>
  
  <Card title="Traffic" icon="chart-line">
    Volumen de requests por segundo
  </Card>
  
  <Card title="Errors" icon="triangle-exclamation">
    Tasa de errores (4xx, 5xx)
  </Card>
  
  <Card title="Saturation" icon="gauge-high">
    Uso de recursos (CPU, memoria, disco)
  </Card>
</CardGroup>

## Umbrales de Alerta

### Latencia (P95)

<Tabs>
  <Tab title="API Gateway">
```yaml
# CloudWatch Alarm - API Gateway Latency
AlarmName: api-gateway-high-latency
MetricName: Latency
Namespace: AWS/ApiGateway
Statistic: p95
Period: 300
EvaluationPeriods: 2
Threshold: 1000  # 1 segundo
ComparisonOperator: GreaterThanThreshold
TreatMissingData: notBreaching

Actions:
  AlarmActions:
    - !Ref CriticalAlertsSNSTopic
  OKActions:
    - !Ref ResolvedAlertsSNSTopic
```
  </Tab>

  <Tab title="Microservicios">
```yaml
# Prometheus Alert - Service Latency
groups:
  - name: latency_alerts
    rules:
      - alert: HighServiceLatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m]))
            by (service, le)
          ) > 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency on {{ $labels.service }}"
          description: "P95 latency is {{ $value }}s"

      - alert: CriticalServiceLatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m]))
            by (service, le)
          ) > 1.0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Critical latency on {{ $labels.service }}"
          description: "P95 latency is {{ $value }}s"
```
  </Tab>
</Tabs>

### Tasa de Errores

<Tabs>
  <Tab title="HTTP 5xx Errors">
```yaml
# Prometheus Alert - Error Rate
groups:
  - name: error_rate_alerts
    rules:
      - alert: HighErrorRate
        expr: |
          sum(rate(http_requests_total{status=~"5.."}[5m]))
          /
          sum(rate(http_requests_total[5m]))
          > 0.01
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }}"

      - alert: CriticalErrorRate
        expr: |
          sum(rate(http_requests_total{status=~"5.."}[5m]))
          /
          sum(rate(http_requests_total[5m]))
          > 0.05
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Critical error rate"
          description: "Error rate is {{ $value | humanizePercentage }}"
```
  </Tab>

  <Tab title="Database Errors">
```yaml
# CloudWatch Alarm - RDS Connection Errors
AlarmName: rds-connection-errors
MetricName: DatabaseConnections
Namespace: AWS/RDS
Statistic: Sum
Period: 300
EvaluationPeriods: 2
Threshold: 10
ComparisonOperator: GreaterThanThreshold

# Prometheus Alert - Query Errors
- alert: HighDatabaseErrorRate
  expr: |
    rate(database_query_errors_total[5m]) > 5
  for: 5m
  labels:
    severity: critical
  annotations:
    summary: "High database error rate"
    description: "{{ $value }} errors/sec"
```
  </Tab>
</Tabs>

### Saturación de Recursos

<Tabs>
  <Tab title="CPU y Memoria">
```yaml
# Kubernetes HPA Metrics
- alert: HighCPUUsage
  expr: |
    sum(rate(container_cpu_usage_seconds_total[5m]))
    by (pod, namespace)
    /
    sum(container_spec_cpu_quota{pod!=""}/container_spec_cpu_period{pod!=""})
    by (pod, namespace)
    > 0.8
  for: 10m
  labels:
    severity: warning
  annotations:
    summary: "High CPU usage on {{ $labels.pod }}"
    description: "CPU usage is {{ $value | humanizePercentage }}"

- alert: HighMemoryUsage
  expr: |
    container_memory_working_set_bytes{pod!=""}
    /
    container_spec_memory_limit_bytes{pod!=""}
    > 0.9
  for: 5m
  labels:
    severity: critical
  annotations:
    summary: "High memory usage on {{ $labels.pod }}"
    description: "CPU usage is {{ $value | humanizePercentage }}"
```
  </Tab>

  <Tab title="Almacenamiento">
```yaml
# RDS Storage Space
- alert: LowDiskSpace
  expr: |
    aws_rds_free_storage_space_bytes
    /
    aws_rds_allocated_storage_bytes
    < 0.15
  for: 10m
  labels:
    severity: warning
  annotations:
    summary: "Low disk space on RDS"
    description: "Only {{ $value | humanizePercentage }} free"

# EBS Volume IOPS
- alert: HighIOPSUsage
  expr: |
    aws_ebs_volume_read_ops_sum + aws_ebs_volume_write_ops_sum
    > 15000
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "High IOPS usage on EBS"
    description: "{{ $value }} IOPS"
```
  </Tab>
</Tabs>

## Configuración de Prometheus

### AlertManager Config

```yaml
global:
  resolve_timeout: 5m
  slack_api_url: ${SLACK_WEBHOOK_URL}

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'slack-notifications'
  
  routes:
    - match:
        severity: critical
      receiver: 'pagerduty-critical'
      continue: true
    
    - match:
        severity: warning
      receiver: 'slack-warnings'

receivers:
  - name: 'slack-notifications'
    slack_configs:
      - channel: '#retrogame-alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true

  - name: 'pagerduty-critical'
    pagerduty_configs:
      - service_key: ${PAGERDUTY_SERVICE_KEY}
        description: '{{ .GroupLabels.alertname }}'

  - name: 'slack-warnings'
    slack_configs:
      - channel: '#retrogame-warnings'
        title: '⚠️ {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
```

## Dashboards de Alertas

### Grafana Dashboard - Overview

```json
{
  "dashboard": {
    "title": "RetroGameCloud - Alerts Overview",
    "panels": [
      {
        "title": "Active Alerts",
        "targets": [
          {
            "expr": "ALERTS{alertstate=\"firing\"}",
            "legendFormat": "{{ alertname }} - {{ severity }}"
          }
        ],
        "type": "stat"
      },
      {
        "title": "Alert Firing Rate",
        "targets": [
          {
            "expr": "rate(alertmanager_alerts_received_total[5m])",
            "legendFormat": "Alerts/sec"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Alerts by Severity",
        "targets": [
          {
            "expr": "sum(ALERTS{alertstate=\"firing\"}) by (severity)",
            "legendFormat": "{{ severity }}"
          }
        ],
        "type": "piechart"
      }
    ]
  }
}
```

## Runbooks de Respuesta

### High Latency Response

<Steps>
  <Step title="Identificar Componente">
    ```bash
    # Ver latencia por servicio
    kubectl top pods -n retrogame-prod
    
    # Consultar métricas Prometheus
    curl -G 'http://prometheus:9090/api/v1/query' \
      --data-urlencode 'query=histogram_quantile(0.95, http_request_duration_seconds_bucket)'
    ```
  </Step>

  <Step title="Verificar Dependencias">
    ```bash
    # Check database connections
    kubectl exec -it <pod> -- psql -c "SELECT count(*) FROM pg_stat_activity;"
    
    # Check Redis latency
    kubectl exec -it redis-0 -- redis-cli --latency
    
    # Check external APIs
    curl -w "@curl-format.txt" -o /dev/null -s https://api.example.com/health
    ```
  </Step>

  <Step title="Escalar si Necesario">
    ```bash
    # Scale up deployment
    kubectl scale deployment game-catalog --replicas=6
    
    # Verificar HPA
    kubectl get hpa
    
    # Forzar rolling restart
    kubectl rollout restart deployment/game-catalog
    ```
  </Step>

  <Step title="Monitorear Mejora">
    Verificar en Grafana que la latencia P95 vuelva a valores normales en los próximos 5 minutos.
  </Step>
</Steps>

### High Error Rate Response

<Steps>
  <Step title="Identificar Tipo de Error">
    ```bash
    # Ver logs recientes con errores
    kubectl logs -l app=auth-service --tail=100 | grep ERROR
    
    # CloudWatch Insights Query
    fields @timestamp, @message
    | filter @message like /ERROR/
    | stats count() by status_code
    | sort count desc
    ```
  </Step>

  <Step title="Verificar Deployment">
    ```bash
    # Verificar deployment reciente
    kubectl rollout history deployment/auth-service
    
    # Si el deployment es sospechoso, rollback
    kubectl rollout undo deployment/auth-service
    ```
  </Step>

  <Step title="Verificar Conexiones Externas">
    ```bash
    # Test database
    kubectl exec -it <pod> -- pg_isready -h $DATABASE_HOST
    
    # Test Redis
    kubectl exec -it <pod> -- redis-cli -h $REDIS_HOST ping
    ```
  </Step>
</Steps>

## Integraciones

### Slack Notifications

```javascript
// Lambda function para notificaciones Slack personalizadas
exports.handler = async (event) => {
  const alarm = JSON.parse(event.Records[0].Sns.Message);
  
  const severity = alarm.Trigger.Threshold > 0.05 ? 'critical' : 'warning';
  const color = severity === 'critical' ? '#dc3545' : '#ffc107';
  
  const message = {
    channel: '#retrogame-alerts',
    attachments: [{
      color: color,
      title: alarm.AlarmName,
      text: alarm.NewStateReason,
      fields: [
        { title: 'Metric', value: alarm.Trigger.MetricName, short: true },
        { title: 'Threshold', value: alarm.Trigger.Threshold.toString(), short: true },
        { title: 'Current Value', value: alarm.NewStateValue, short: true },
        { title: 'Region', value: process.env.AWS_REGION, short: true }
      ],
      footer: 'RetroGameCloud Monitoring',
      ts: Math.floor(Date.now() / 1000)
    }]
  };
  
  await fetch(process.env.SLACK_WEBHOOK_URL, {
    method: 'POST',
    body: JSON.stringify(message)
  });
};
```

### PagerDuty Integration

```python
# Lambda function para escalar a PagerDuty
import os
import json
import requests

def lambda_handler(event, context):
    alarm = json.loads(event['Records'][0]['Sns']['Message'])
    
    # Solo escalar si es crítico
    if alarm['NewStateValue'] == 'ALARM' and 'critical' in alarm['AlarmName'].lower():
        payload = {
            'routing_key': os.environ['PAGERDUTY_ROUTING_KEY'],
            'event_action': 'trigger',
            'payload': {
                'summary': alarm['AlarmName'],
                'severity': 'critical',
                'source': 'AWS CloudWatch',
                'custom_details': {
                    'metric': alarm['Trigger']['MetricName'],
                    'threshold': alarm['Trigger']['Threshold'],
                    'current_value': alarm['NewStateValue']
                }
            }
        }
        
        requests.post(
            'https://events.pagerduty.com/v2/enqueue',
            json=payload
        )
```

## Mejores Prácticas

<AccordionGroup>
  <Accordion title="Reducir Falsos Positivos">
    - Ajustar períodos de evaluación apropiados
    - Usar `for: Xm` en Prometheus para evitar alertas transitorias
    - Implementar thresholds dinámicos basados en tendencias
    - Correlacionar múltiples métricas antes de alertar
  </Accordion>

  <Accordion title="Alert Fatigue Prevention">
    - Limitar alertas a problemas que requieren acción humana
    - Agrupar alertas relacionadas
    - Implementar auto-remediation cuando sea posible
    - Revisar y refinar umbrales regularmente
  </Accordion>

  <Accordion title="On-Call Best Practices">
    - Mantener runbooks actualizados
    - Rotar guardia entre equipo
    - Post-mortem después de incidentes mayores
    - Documentar resoluciones comunes
  </Accordion>
</AccordionGroup>

## Métricas de Alertas

### KPIs de Sistema de Alertas

```prometheus
# Mean Time To Detect (MTTD)
avg(timestamp(ALERTS{alertstate="firing"}) - timestamp(incident_start))

# Mean Time To Resolve (MTTR)
avg(timestamp(ALERTS{alertstate="resolved"}) - timestamp(ALERTS{alertstate="firing"}))

# Alert Noise Ratio
sum(rate(alertmanager_alerts_invalid_total[24h]))
/
sum(rate(alertmanager_alerts_received_total[24h]))
```

<Warning>
Revisar métricas de alertas mensualmente. MTTD objetivo: < 5 minutos. MTTR objetivo: < 30 minutos para incidentes críticos.
</Warning>

---

Un sistema de alertas efectivo es clave para mantener RetroGameCloud altamente disponible y con excelente experiencia de usuario.
