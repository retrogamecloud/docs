---
title: 4.7. Monitoreo y Observabilidad
sidebarTitle: 5.8. Monitoreo y Observabilidad
description: Stack completo de observabilidad con Prometheus y Grafana.
  Golden Signals y dashboards pre-configurados para Retro Game Hub
icon: chart-line
---

## 4.7.1. Stack de Monitoreo y Observabilidad

Retro Game Hub implementa observabilidad completa siguiendo las mejores pr谩cticas SRE con Golden Signals y logging estructurado.

<CardGroup cols={3}>
  <Card title="Prometheus" icon="chart-line">
    **M茅tricas**

    Recolecci贸n y almacenamiento de m茅tricas de aplicaci贸n y Golden Signals
  </Card>

  <Card title="Grafana" icon="chart-area">
    **Visualizaci贸n**

    Dashboards SLI/SLO y an谩lisis de m茅tricas
  </Card>

  <Card title="AlertManager" icon="bell">
    **Alertas**

    Notificaciones a Slack para eventos cr铆ticos
  </Card>
</CardGroup>

## 4.7.2. Arquitectura de Observabilidad

```mermaid
graph TD
    A[Route53 DNS<br/>retrogamehub.games] --> B[Application Load Balancer<br/>SSL/TLS Termination]
    B --> C[Grafana Pod<br/>:3000]
    B --> D[Prometheus Pod<br/>:9090]
    B --> E[AlertManager Pod<br/>:9093]

    D --> F[Service Discovery]
    F --> G[node-exporter<br/>Node Metrics]
    F --> H[kube-state-metrics<br/>K8s Metrics]
    F --> I[Backend Pods<br/>/metrics endpoints]

    D --> E
    E --> J[Slack<br/>#notificacionesrgh]
    C --> D

```

## 4.7.3. Golden Signals Implementation

### 4.7.3.1. Latencia (Latency)

```yaml
# prometheus-rules.yaml
groups:
  - name: golden-signals-latency
    rules:
      - record: retrogame:request_duration_seconds:rate5m
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m]))
            by (service, method, le)
          )

      - alert: HighLatency
        expr: |
          retrogame:request_duration_seconds:rate5m > 0.5
        for: 2m
        labels:
          severity: warning
          team: sre
        annotations:
          summary: "High latency on {{ $labels.service }}"
          description: "95th percentile latency is {{ $value }}s"

```

### 4.7.3.2. Tr谩fico (Traffic)

```yaml
  - name: golden-signals-traffic
    rules:
      - record: retrogame:request_rate:rate5m
        expr: |
          sum(rate(http_requests_total[5m]))
          by (service, method, status)

      - alert: TrafficSpike
        expr: |
          (
            retrogame:request_rate:rate5m >
            (retrogame:request_rate:rate1h offset 1h) * 2
          )
        for: 5m
        labels:
          severity: info

```

### 4.7.3.3. Errores (Errors)

```yaml
  - name: golden-signals-errors
    rules:
      - record: retrogame:error_rate:rate5m
        expr: |
          sum(rate(http_requests_total{status=~"5.."}[5m]))
          by (service) /
          sum(rate(http_requests_total[5m]))
          by (service)

      - alert: HighErrorRate
        expr: retrogame:error_rate:rate5m > 0.05
        for: 2m
        labels:
          severity: critical

```

### 4.7.3.4. Saturaci贸n (Saturation)

```yaml
  - name: golden-signals-saturation
    rules:
      - alert: HighCPU
        expr: |
          avg(rate(container_cpu_usage_seconds_total[5m]))
          by (pod) > 0.8
        for: 5m

      - alert: HighMemory
        expr: |
          container_memory_usage_bytes /
          container_spec_memory_limit_bytes > 0.85
        for: 2m

```

## 4.7.4. Instrumentaci贸n de Microservicios

### 4.7.4.1. Node.js Prometheus Client

```javascript
// src/shared/monitoring/prometheus.js
const prometheus = require('prom-client');
const logger = require('./logger');

class PrometheusMetrics {
  constructor() {
    // Registry personalizado
    this.register = new prometheus.Registry();

    // M茅tricas por defecto
    prometheus.collectDefaultMetrics({
      register: this.register,
      prefix: 'retrogame_',
      gcDurationBuckets: [0.001, 0.01, 0.1, 1, 2, 5]
    });

    // Custom metrics
    this.httpDuration = new prometheus.Histogram({
      name: 'retrogame_http_request_duration_seconds',
      help: 'Duration of HTTP requests in seconds',
      labelNames: ['service', 'method', 'route', 'status'],
      buckets: [0.1, 0.3, 0.5, 0.7, 1, 3, 5, 7, 10],
      registers: [this.register]
    });

    this.httpRequests = new prometheus.Counter({
      name: 'retrogame_http_requests_total',
      help: 'Total number of HTTP requests',
      labelNames: ['service', 'method', 'route', 'status'],
      registers: [this.register]
    });

    this.activeConnections = new prometheus.Gauge({
      name: 'retrogame_active_connections',
      help: 'Number of active WebSocket connections',
      labelNames: ['service'],
      registers: [this.register]
    });

    this.businessMetrics = {
      gamesPlayed: new prometheus.Counter({
        name: 'retrogame_games_played_total',
        help: 'Total games played',
        labelNames: ['game_type', 'mode'],
        registers: [this.register]
      }),

      usersOnline: new prometheus.Gauge({
        name: 'retrogame_users_online',
        help: 'Current users online',
        registers: [this.register]
      }),

      matchmakingTime: new prometheus.Histogram({
        name: 'retrogame_matchmaking_duration_seconds',
        help: 'Time taken for matchmaking',
        buckets: [1, 2, 5, 10, 30, 60],
        registers: [this.register]
      })
    };
  }

  middleware() {
    return (req, res, next) => {
      const start = Date.now();

      res.on('finish', () => {
        const duration = (Date.now() - start) / 1000;

        const labels = {
          service: process.env.SERVICE_NAME,
          method: req.method,
          route: req.route?.path || 'unknown',
          status: res.statusCode
        };

        this.httpDuration.observe(labels, duration);
        this.httpRequests.inc(labels);
      });

      next();
    };
  }

  trackBusinessMetric(metric, labels = {}, value = 1) {
    try {
      if (this.businessMetrics[metric]) {
        if (this.businessMetrics[metric].inc) {
          this.businessMetrics[metric].inc(labels, value);
        } else {
          this.businessMetrics[metric].set(labels, value);
        }
      }
    } catch (error) {
      logger.error('Error tracking business metric', { metric, error });
    }
  }

  getMetrics() {
    return this.register.metrics();
  }
}

module.exports = new PrometheusMetrics();

```

### 4.7.4.2. Middleware Express

```javascript
// src/shared/monitoring/middleware.js
const prometheusMetrics = require('./prometheus');
const logger = require('./logger');
const { v4: uuidv4 } = require('uuid');

const monitoringMiddleware = (req, res, next) => {
  // Correlation ID
  req.correlationId = req.headers['x-correlation-id'] || uuidv4();
  res.setHeader('X-Correlation-ID', req.correlationId);

  // Request logging
  logger.info('Request started', {
    correlationId: req.correlationId,
    method: req.method,
    url: req.url,
    userAgent: req.headers['user-agent'],
    ip: req.ip
  });

  // Response logging
  const originalSend = res.send;
  res.send = function(data) {
    logger.info('Request completed', {
      correlationId: req.correlationId,
      status: res.statusCode,
      responseTime: Date.now() - req.startTime
    });

    return originalSend.call(this, data);
  };

  req.startTime = Date.now();
  next();
};

module.exports = {
  monitoring: monitoringMiddleware,
  prometheus: prometheusMetrics.middleware()
};

```

## 4.7.5. Distributed Tracing con Jaeger

### 4.7.5.1. Configuraci贸n OpenTelemetry

```javascript
// src/shared/tracing/tracer.js
const { NodeSDK } = require('@opentelemetry/sdk-node');
const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node');
const { JaegerExporter } = require('@opentelemetry/exporter-jaeger');
const { Resource } = require('@opentelemetry/resources');
const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions');

const jaegerExporter = new JaegerExporter({
  endpoint: process.env.JAEGER_ENDPOINT || 'http://jaeger:14268/api/traces',
});

const sdk = new NodeSDK({
  resource: new Resource({
    [SemanticResourceAttributes.SERVICE_NAME]: process.env.SERVICE_NAME,
    [SemanticResourceAttributes.SERVICE_VERSION]: process.env.SERVICE_VERSION,
    [SemanticResourceAttributes.DEPLOYMENT_ENVIRONMENT]: process.env.NODE_ENV,
  }),
  traceExporter: jaegerExporter,
  instrumentations: [
    getNodeAutoInstrumentations({
      '@opentelemetry/instrumentation-http': {
        enabled: true,
        requestHook: (span, request) => {
          span.setAttributes({
            'http.request.correlation_id': request.headers['x-correlation-id']
          });
        }
      }
    })
  ],
});

sdk.start();

module.exports = sdk;

```

### 4.7.5.2. Custom Spans

```javascript
// src/shared/tracing/spans.js
const { trace, context } = require('@opentelemetry/api');
const tracer = trace.getTracer('retrogame-tracer');

class TracingHelper {
  static async executeWithSpan(name, operation, attributes = {}) {
    return tracer.startActiveSpan(name, { attributes }, async (span) => {
      try {
        const result = await operation(span);
        span.setStatus({ code: 2 }); // OK
        return result;
      } catch (error) {
        span.recordException(error);
        span.setStatus({ code: 2, message: error.message }); // ERROR
        throw error;
      } finally {
        span.end();
      }
    });
  }

  static getCurrentSpan() {
    return trace.getActiveSpan();
  }

  static addEvent(name, attributes = {}) {
    const span = this.getCurrentSpan();
    if (span) {
      span.addEvent(name, attributes);
    }
  }
}

module.exports = TracingHelper;

```

## 4.7.6. Logging Estructurado

### 4.7.6.1. Winston Logger

```javascript
// src/shared/monitoring/logger.js
const winston = require('winston');
const { CloudWatchLogsTransport } = require('winston-aws-cloudwatch');

const logger = winston.createLogger({
  level: process.env.LOG_LEVEL || 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.errors({ stack: true }),
    winston.format.json(),
    winston.format.printf(({ timestamp, level, message, ...meta }) => {
      return JSON.stringify({
        timestamp,
        level,
        service: process.env.SERVICE_NAME,
        version: process.env.SERVICE_VERSION,
        environment: process.env.NODE_ENV,
        message,
        ...meta
      });
    })
  ),
  transports: [
    new winston.transports.Console({
      format: winston.format.combine(
        winston.format.colorize(),
        winston.format.simple()
      )
    }),

    new CloudWatchLogsTransport({
      logGroupName: `/retrogame/${process.env.SERVICE_NAME}`,
      logStreamName: `${process.env.SERVICE_NAME}-${new Date().getTime()}`,
      awsRegion: process.env.AWS_REGION,
      retentionInDays: 30
    })
  ]
});

module.exports = logger;

```

---

## 4.7.7. AlertManager - Sistema de Alertas

### 4.7.7.1. Descripci贸n

AlertManager gestiona las alertas generadas por Prometheus, agrup谩ndolas, deduplic谩ndolas y enviando notificaciones a **Slack** en el canal `#notificacionesrgh`.

<CardGroup cols={3}>
  <Card title="Agrupaci贸n" icon="layer-group">
    Agrupa alertas similares para evitar spam
  </Card>

  <Card title="Inhibici贸n" icon="ban">
    Suprime alertas de baja severidad cuando hay cr铆ticas
  </Card>

  <Card title="Enrutamiento" icon="route">
    Diferentes canales seg煤n severidad
  </Card>
</CardGroup>

### 4.7.7.2. Configuraci贸n de Rutas

```yaml
route:
  receiver: default
  group_by: ['alertname', 'cluster', 'namespace', 'pod']
  group_wait: 10s        # Espera antes de enviar grupo
  group_interval: 5m     # Tiempo entre grupos
  repeat_interval: 4h    # Tiempo antes de reenviar

  routes:
    # Alertas cr铆ticas - atenci贸n inmediata
    - receiver: critical-alerts
      match:
        severity: critical
      group_wait: 10s
      repeat_interval: 1h
    
    # Alertas de advertencia
    - receiver: warning-alerts
      match:
        severity: warning
      group_wait: 30s
      repeat_interval: 4h
    
    # Alertas de aplicaci贸n
    - receiver: app-alerts
      match_re:
        namespace: retrogame
      group_wait: 10s
      repeat_interval: 2h
```

### 4.7.7.3. Receptores Slack

#### Canal: `#notificacionesrgh`

Todos los receptores env铆an notificaciones al mismo canal de Slack usando el **Slack Bot Token** almacenado en AWS Secrets Manager.

<Tabs>
  <Tab title="Critical Alerts">
```yaml
receivers:
  - name: critical-alerts
    slack_configs:
      - api_url: https://slack.com/api/chat.postMessage
        http_config:
          authorization:
            credentials: <SLACK_BOT_TOKEN>
        channel: "#notificacionesrgh"
        title: " [CRTICO] {{ .GroupLabels.alertname }}"
        text: |
          {{ range .Alerts }}
          *Alerta:* {{ .Annotations.summary }}
          *Descripci贸n:* {{ .Annotations.description }}
          *Namespace:* {{ .Labels.namespace }}
          *Pod:* {{ .Labels.pod }}
          {{ end }}
        send_resolved: true
        color: danger
```

**Ejemplo de mensaje:**
>  **[CRTICO] PodCrashLooping**
> 
> **Alerta:** Backend pod est谩 reiniciando constantemente
> **Descripci贸n:** Pod backend-deployment-abc123 ha reiniciado 5 veces en 5 minutos
> **Namespace:** retrogame
> **Pod:** backend-deployment-abc123
  </Tab>

  <Tab title="Warning Alerts">
```yaml
  - name: warning-alerts
    slack_configs:
      - api_url: https://slack.com/api/chat.postMessage
        http_config:
          authorization:
            credentials: <SLACK_BOT_TOKEN>
        channel: "#notificacionesrgh"
        title: "锔 [ADVERTENCIA] {{ .GroupLabels.alertname }}"
        text: |
          {{ range .Alerts }}
          *Alerta:* {{ .Annotations.summary }}
          *Descripci贸n:* {{ .Annotations.description }}
          {{ end }}
        send_resolved: true
        color: warning
```

**Ejemplo de mensaje:**
> 锔 **[ADVERTENCIA] HighMemoryUsage**
> 
> **Alerta:** Uso de memoria elevado
> **Descripci贸n:** Backend usando 85% de memoria asignada
  </Tab>

  <Tab title="App Alerts">
```yaml
  - name: app-alerts
    slack_configs:
      - api_url: https://slack.com/api/chat.postMessage
        http_config:
          authorization:
            credentials: <SLACK_BOT_TOKEN>
        channel: "#notificacionesrgh"
        title: " [RETROGAME] {{ .GroupLabels.alertname }}"
        text: |
          {{ range .Alerts }}
          *Servicio:* {{ .Labels.service }}
          *Alerta:* {{ .Annotations.summary }}
          *Descripci贸n:* {{ .Annotations.description }}
          {{ end }}
        send_resolved: true
        color: "#FF6B6B"
```

**Ejemplo de mensaje:**
>  **[RETROGAME] HighErrorRate**
> 
> **Servicio:** backend
> **Alerta:** Tasa de error elevada
> **Descripci贸n:** 5% de requests devuelven errores 5xx
  </Tab>
</Tabs>

### 4.7.7.4. Reglas de Inhibici贸n

Las alertas cr铆ticas **suprimen** las de menor severidad para evitar ruido:

```yaml
inhibit_rules:
  - source_match:
      severity: critical
    target_match:
      severity: warning
    equal: ['alertname', 'namespace', 'pod']
```

**Ejemplo:**
- Si `PodCrashLooping` (critical) est谩 activa
- Se suprime `HighMemoryUsage` (warning) del mismo pod
- **Raz贸n:** El crash es m谩s urgente que el uso de memoria

### 4.7.7.5. Configuraci贸n de Secretos

#### AWS Secrets Manager

El Slack Bot Token se almacena en Secrets Manager:

```bash
# Crear secret
aws secretsmanager create-secret \
  --name prod/retrogame/slack-bot-token \
  --description "Slack Bot Token para AlertManager" \
  --secret-string "YOUR_SLACK_BOT_TOKEN_HERE"

# Verificar
aws secretsmanager get-secret-value \
  --secret-id prod/retrogame/slack-bot-token \
  --query SecretString \
  --output text
```

#### Kubernetes Secret

El token se sincroniza autom谩ticamente desde Secrets Manager:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-slack-webhook
  namespace: monitoring
type: Opaque
data:
  webhook_url: <base64-encoded-token>
```

### 4.7.7.6. Ejemplos de Alertas Comunes

<AccordionGroup>
  <Accordion title=" PodCrashLooping (Critical)">
    **Condici贸n:**
    ```promql
    rate(kube_pod_container_status_restarts_total[15m]) > 0
    ```

    **Notificaci贸n Slack:**
    >  **[CRTICO] PodCrashLooping**
    > 
    > **Alerta:** Pod reiniciando constantemente
    > **Descripci贸n:** backend-deployment-abc123 ha reiniciado 5 veces en 15 minutos
    > **Namespace:** retrogame
    > **Pod:** backend-deployment-abc123
    > 
    > **Acciones:**
    > ```bash
    > kubectl logs -n retrogame backend-deployment-abc123 --previous
    > kubectl describe pod -n retrogame backend-deployment-abc123
    > ```
  </Accordion>

  <Accordion title="锔 HighErrorRate (Warning)">
    **Condici贸n:**
    ```promql
    (
      sum(rate(http_requests_total{status=~"5.."}[5m]))
      /
      sum(rate(http_requests_total[5m]))
    ) > 0.05
    ```

    **Notificaci贸n Slack:**
    > 锔 **[ADVERTENCIA] HighErrorRate**
    > 
    > **Alerta:** Tasa de error elevada en backend
    > **Descripci贸n:** 7.3% de requests devuelven errores 5xx
    > 
    > **Dashboard:** https://retrogamehub.games/grafana/d/errors
  </Accordion>

  <Accordion title=" DatabaseConnectionPoolExhausted (App)">
    **Condici贸n:**
    ```promql
    pg_pool_waiting_count > 5
    ```

    **Notificaci贸n Slack:**
    >  **[RETROGAME] DatabaseConnectionPoolExhausted**
    > 
    > **Servicio:** backend
    > **Alerta:** Connection pool de PostgreSQL agotado
    > **Descripci贸n:** 8 conexiones esperando en cola
    > 
    > **Soluci贸n:** Escalar replicas o aumentar pool size
  </Accordion>
</AccordionGroup>

### 4.7.7.7. Testing de Alertas

#### Silenciar Alerta Temporalmente

```bash
# Silenciar por 1 hora
curl -X POST https://retrogamehub.games/alertmanager/api/v1/silences \
  -H "Content-Type: application/json" \
  -d '{
    "matchers": [
      {"name": "alertname", "value": "HighMemoryUsage", "isRegex": false}
    ],
    "startsAt": "2025-12-07T10:00:00Z",
    "endsAt": "2025-12-07T11:00:00Z",
    "createdBy": "sre-team",
    "comment": "Mantenimiento programado"
  }'
```

#### Generar Alerta de Prueba

```bash
# Escalar backend a 0 replicas para generar alerta
kubectl scale deployment/backend-deployment -n retrogame --replicas=0

# Esperar ~2 minutos
# AlertManager enviar谩: " [CRTICO] PodNotReady"

# Restaurar
kubectl scale deployment/backend-deployment -n retrogame --replicas=2
```

#### Verificar Estado

```bash
# Ver alertas activas
curl https://retrogamehub.games/alertmanager/api/v1/alerts | jq

# Ver silencios activos
curl https://retrogamehub.games/alertmanager/api/v1/silences | jq
```

### 4.7.7.8. Acceso Web

**URL:** `https://retrogamehub.games/alertmanager`

**Autenticaci贸n:** OAuth2-Proxy (GitHub)

**Funcionalidades:**
- Ver alertas activas y resueltas
- Crear silencios temporales
- Ver configuraci贸n de rutas
- Historial de notificaciones

---

## 4.7.8. Dashboards Grafana

### 4.7.7.1. Service Overview Dashboard

```json
{
  "dashboard": {
    "title": "Retro Game Hub - Service Overview",
    "panels": [
      {
        "title": "Request Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(rate(retrogame_http_requests_total[5m]))",
            "legendFormat": "RPS"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "reqps"
          }
        }
      },
      {
        "title": "Error Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(rate(retrogame_http_requests_total{status=~\"5..\"}[5m])) / sum(rate(retrogame_http_requests_total[5m]))",
            "legendFormat": "Error %"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "thresholds": {
              "steps": [
                { "color": "green", "value": null },
                { "color": "yellow", "value": 0.01 },
                { "color": "red", "value": 0.05

```