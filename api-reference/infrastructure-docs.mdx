---
title: "6.3. Infrastructure - Documentación Terraform"
description: "Estructura y configuración de infraestructura como código con Terraform en AWS"
icon: "cloud"
---

## Visión General

La infraestructura de Retro Game Hub está completamente definida como código usando **Terraform** y desplegada en **AWS**. Esto incluye el clúster EKS, redes, bases de datos, CDN, y servicios de monitoreo.

## Estructura del Repositorio

```
infrastructure/
├── terraform/
│   ├── eks/                    # Clúster Kubernetes
│   │   ├── main.tf
│   │   ├── vpc.tf
│   │   ├── eks_cluster.tf
│   │   ├── node_groups.tf
│   │   ├── ingress_monitoring.tf
│   │   ├── s3-cdn.tf
│   │   ├── oauth2_proxy.tf
│   │   ├── route53.tf
│   │   ├── variables.tf
│   │   └── outputs.tf
│   │
│   ├── rds/                    # Bases de datos
│   │   ├── main.tf
│   │   ├── postgresql.tf
│   │   ├── security_groups.tf
│   │   └── outputs.tf
│   │
│   ├── elasticache/            # Redis
│   │   ├── main.tf
│   │   ├── redis.tf
│   │   └── outputs.tf
│   │
│   └── monitoring/             # Observabilidad
│       ├── main.tf
│       ├── cloudwatch.tf
│       └── sns.tf
│
├── cdn/                        # CDN NGINX
│   ├── Dockerfile
│   ├── cdn.conf
│   ├── img/
│   └── juegos/
│
├── kong/                       # API Gateway
│   └── kong.yml
│
└── README.md
```

## Módulos Terraform

### EKS Cluster

<Accordion title="main.tf - Configuración Principal">
```hcl
terraform {
  required_version = ">= 1.0"
  
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.23"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "~> 2.11"
    }
  }
  
  backend "s3" {
    bucket         = "retrogame-terraform-state"
    key            = "eks/terraform.tfstate"
    region         = "us-east-1"
    encrypt        = true
    dynamodb_table = "terraform-lock"
  }
}

provider "aws" {
  region = var.aws_region
  
  default_tags {
    tags = {
      Project     = "RetroGameHub"
      Environment = var.environment
      ManagedBy   = "Terraform"
    }
  }
}

data "aws_availability_zones" "available" {
  state = "available"
}

data "aws_caller_identity" "current" {}
```
</Accordion>

<Accordion title="vpc.tf - Red VPC">
```hcl
module "vpc" {
  source  = "terraform-aws-modules/vpc/aws"
  version = "~> 5.0"

  name = "${var.cluster_name}-vpc"
  cidr = var.vpc_cidr

  azs             = slice(data.aws_availability_zones.available.names, 0, 3)
  private_subnets = var.private_subnet_cidrs
  public_subnets  = var.public_subnet_cidrs

  enable_nat_gateway   = true
  single_nat_gateway   = var.environment == "dev"
  enable_dns_hostnames = true
  enable_dns_support   = true

  # Tags requeridos para EKS
  public_subnet_tags = {
    "kubernetes.io/role/elb"                    = "1"
    "kubernetes.io/cluster/${var.cluster_name}" = "shared"
  }

  private_subnet_tags = {
    "kubernetes.io/role/internal-elb"           = "1"
    "kubernetes.io/cluster/${var.cluster_name}" = "shared"
  }

  tags = {
    Name = "${var.cluster_name}-vpc"
  }
}
```
</Accordion>

<Accordion title="eks_cluster.tf - Clúster EKS">
```hcl
module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "~> 19.0"

  cluster_name    = var.cluster_name
  cluster_version = var.kubernetes_version

  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnets

  # Acceso público al API
  cluster_endpoint_public_access = true
  cluster_endpoint_private_access = true

  # Addons
  cluster_addons = {
    coredns = {
      most_recent = true
    }
    kube-proxy = {
      most_recent = true
    }
    vpc-cni = {
      most_recent = true
    }
    aws-ebs-csi-driver = {
      most_recent = true
    }
  }

  # OIDC Provider para IAM Roles
  enable_irsa = true

  # Cluster Security Group
  cluster_security_group_additional_rules = {
    ingress_nodes_ephemeral_ports = {
      description                = "Nodes ephemeral ports"
      protocol                   = "tcp"
      from_port                  = 1025
      to_port                    = 65535
      type                       = "ingress"
      source_node_security_group = true
    }
  }

  # Node Security Group
  node_security_group_additional_rules = {
    ingress_self_all = {
      description = "Node to node all ports/protocols"
      protocol    = "-1"
      from_port   = 0
      to_port     = 0
      type        = "ingress"
      self        = true
    }
    egress_all = {
      description      = "Node all egress"
      protocol         = "-1"
      from_port        = 0
      to_port          = 0
      type             = "egress"
      cidr_blocks      = ["0.0.0.0/0"]
      ipv6_cidr_blocks = ["::/0"]
    }
  }

  tags = {
    Name = var.cluster_name
  }
}
```
</Accordion>

<Accordion title="node_groups.tf - Grupos de Nodos">
```hcl
module "eks_managed_node_group" {
  source  = "terraform-aws-modules/eks/aws//modules/eks-managed-node-group"
  version = "~> 19.0"

  for_each = var.node_groups

  name            = each.key
  cluster_name    = module.eks.cluster_name
  cluster_version = var.kubernetes_version

  subnet_ids = module.vpc.private_subnets

  # Configuración de instancias
  instance_types = each.value.instance_types
  capacity_type  = each.value.capacity_type

  # Scaling
  min_size     = each.value.min_size
  max_size     = each.value.max_size
  desired_size = each.value.desired_size

  # Disco
  block_device_mappings = {
    xvda = {
      device_name = "/dev/xvda"
      ebs = {
        volume_size           = each.value.disk_size
        volume_type           = "gp3"
        iops                  = 3000
        throughput            = 125
        encrypted             = true
        delete_on_termination = true
      }
    }
  }

  # Labels
  labels = each.value.labels

  # Taints
  taints = each.value.taints

  # Tags
  tags = merge(
    {
      Name = each.key
    },
    each.value.tags
  )
}
```
</Accordion>

<Accordion title="ingress_monitoring.tf - Ingress y Monitoreo">
```hcl
# Namespace para monitoreo
resource "kubernetes_namespace" "monitoring" {
  metadata {
    name = "monitoring"
  }
}

# NGINX Ingress Controller
resource "helm_release" "nginx_ingress" {
  name       = "ingress-nginx"
  repository = "https://kubernetes.github.io/ingress-nginx"
  chart      = "ingress-nginx"
  version    = "4.8.3"
  namespace  = "ingress-nginx"
  create_namespace = true

  set {
    name  = "controller.service.type"
    value = "LoadBalancer"
  }

  set {
    name  = "controller.service.annotations.service\\.beta\\.kubernetes\\.io/aws-load-balancer-type"
    value = "nlb"
  }

  set {
    name  = "controller.metrics.enabled"
    value = "true"
  }

  set {
    name  = "controller.podAnnotations.prometheus\\.io/scrape"
    value = "true"
  }
}

# Prometheus Stack
resource "helm_release" "prometheus" {
  name       = "prometheus"
  repository = "https://prometheus-community.github.io/helm-charts"
  chart      = "kube-prometheus-stack"
  version    = "54.2.2"
  namespace  = kubernetes_namespace.monitoring.metadata[0].name

  values = [
    file("${path.module}/values/prometheus-values.yaml")
  ]

  set {
    name  = "grafana.adminPassword"
    value = var.grafana_admin_password
  }

  depends_on = [module.eks]
}

# Ingress para Grafana con OAuth2
resource "kubernetes_ingress_v1" "grafana" {
  metadata {
    name      = "grafana-ingress"
    namespace = kubernetes_namespace.monitoring.metadata[0].name
    annotations = {
      "kubernetes.io/ingress.class"                       = "nginx"
      "cert-manager.io/cluster-issuer"                   = "letsencrypt-prod"
      "nginx.ingress.kubernetes.io/auth-url"             = "https://$host/oauth2/auth"
      "nginx.ingress.kubernetes.io/auth-signin"          = "https://$host/oauth2/start?rd=$escaped_request_uri"
      "nginx.ingress.kubernetes.io/auth-response-headers" = "X-Auth-Request-User,X-Auth-Request-Email"
    }
  }

  spec {
    tls {
      hosts       = ["grafana.${var.domain_name}"]
      secret_name = "grafana-tls"
    }

    rule {
      host = "grafana.${var.domain_name}"
      http {
        path {
          path      = "/"
          path_type = "Prefix"
          backend {
            service {
              name = "prometheus-grafana"
              port {
                number = 80
              }
            }
          }
        }
      }
    }
  }
}
```
</Accordion>

<Accordion title="s3-cdn.tf - S3 y CloudFront">
```hcl
# Bucket para juegos
resource "aws_s3_bucket" "games_cdn" {
  bucket = "${var.project_name}-games-cdn"
  
  tags = {
    Name        = "Games CDN"
    Purpose     = "Store game files and static assets"
  }
}

# Bucket para logs
resource "aws_s3_bucket" "cdn_logs" {
  bucket = "${var.project_name}-cdn-logs"
  
  tags = {
    Name    = "CDN Logs"
    Purpose = "CloudFront access logs"
  }
}

# Configuración de logging
resource "aws_s3_bucket_logging" {
  bucket = aws_s3_bucket.games_cdn.id

  target_bucket = aws_s3_bucket.cdn_logs.id
  target_prefix = "cdn/"
}

# Origin Access Control
resource "aws_cloudfront_origin_access_control" "games_cdn" {
  name                              = "${var.project_name}-games-cdn-oac"
  origin_access_control_origin_type = "s3"
  signing_behavior                  = "always"
  signing_protocol                  = "sigv4"
}

# CloudFront Distribution
resource "aws_cloudfront_distribution" "games_cdn" {
  enabled             = true
  is_ipv6_enabled     = true
  comment             = "CDN for Retro Game Hub"
  default_root_object = "index.html"
  price_class         = "PriceClass_100"

  origin {
    domain_name              = aws_s3_bucket.games_cdn.bucket_regional_domain_name
    origin_id                = "S3-${aws_s3_bucket.games_cdn.id}"
    origin_access_control_id = aws_cloudfront_origin_access_control.games_cdn.id
  }

  default_cache_behavior {
    allowed_methods  = ["GET", "HEAD", "OPTIONS"]
    cached_methods   = ["GET", "HEAD"]
    target_origin_id = "S3-${aws_s3_bucket.games_cdn.id}"

    forwarded_values {
      query_string = false
      cookies {
        forward = "none"
      }
    }

    viewer_protocol_policy = "redirect-to-https"
    min_ttl                = 0
    default_ttl            = 3600
    max_ttl                = 86400
    compress               = true
  }

  # Cache behavior para juegos (.jsdos)
  ordered_cache_behavior {
    path_pattern     = "/juegos/*.jsdos"
    allowed_methods  = ["GET", "HEAD"]
    cached_methods   = ["GET", "HEAD"]
    target_origin_id = "S3-${aws_s3_bucket.games_cdn.id}"

    forwarded_values {
      query_string = false
      cookies {
        forward = "none"
      }
    }

    viewer_protocol_policy = "redirect-to-https"
    min_ttl                = 0
    default_ttl            = 604800  # 7 días
    max_ttl                = 31536000
    compress               = false   # .jsdos ya está comprimido
  }

  restrictions {
    geo_restriction {
      restriction_type = "none"
    }
  }

  viewer_certificate {
    cloudfront_default_certificate = true
  }

  logging_config {
    bucket          = aws_s3_bucket.cdn_logs.bucket_domain_name
    include_cookies = false
    prefix          = "cloudfront/"
  }

  tags = {
    Name = "${var.project_name}-cdn"
  }
}

# Upload de archivos estáticos
resource "null_resource" "upload_static_files" {
  triggers = {
    always_run = timestamp()
  }

  provisioner "local-exec" {
    command = <<EOF
      aws s3 sync ../cdn/juegos/ s3://${aws_s3_bucket.games_cdn.bucket}/juegos/ \
        --exclude "*" --include "*.jsdos" \
        --cache-control "max-age=604800"
      
      aws s3 sync ../cdn/img/ s3://${aws_s3_bucket.games_cdn.bucket}/img/ \
        --cache-control "max-age=604800"
    EOF
  }

  depends_on = [aws_s3_bucket.games_cdn]
}
```
</Accordion>

<Accordion title="oauth2_proxy.tf - Autenticación OAuth2">
```hcl
# Namespace para OAuth2
resource "kubernetes_namespace" "oauth2_proxy" {
  metadata {
    name = "oauth2-proxy"
  }
}

# ConfigMap
resource "kubernetes_config_map" "oauth2_proxy" {
  metadata {
    name      = "oauth2-proxy-config"
    namespace = kubernetes_namespace.oauth2_proxy.metadata[0].name
  }

  data = {
    oauth2_proxy_cfg = <<-EOT
      provider = "github"
      github_org = "${var.github_org}"
      github_team = ""
      email_domains = [ "*" ]
      cookie_secure = true
      cookie_httponly = true
      cookie_expire = "168h"
      cookie_refresh = "1h"
      pass_authorization_header = true
      pass_access_token = true
      pass_user_headers = true
      set_authorization_header = true
      set_xauthrequest = true
      cookie_name = "_oauth2_proxy"
      whitelist_domains = ".${var.domain_name}"
    EOT
  }
}

# Secret
resource "kubernetes_secret" "oauth2_proxy" {
  metadata {
    name      = "oauth2-proxy-secret"
    namespace = kubernetes_namespace.oauth2_proxy.metadata[0].name
  }

  data = {
    client_id     = var.github_oauth_client_id
    client_secret = var.github_oauth_client_secret
    cookie_secret = random_password.oauth2_cookie_secret.result
  }
}

resource "random_password" "oauth2_cookie_secret" {
  length  = 32
  special = true
}

# Deployment
resource "kubernetes_deployment" "oauth2_proxy" {
  metadata {
    name      = "oauth2-proxy"
    namespace = kubernetes_namespace.oauth2_proxy.metadata[0].name
  }

  spec {
    replicas = 2

    selector {
      match_labels = {
        app = "oauth2-proxy"
      }
    }

    template {
      metadata {
        labels = {
          app = "oauth2-proxy"
        }
      }

      spec {
        container {
          name  = "oauth2-proxy"
          image = "quay.io/oauth2-proxy/oauth2-proxy:v7.6.0"

          args = [
            "--config=/etc/oauth2_proxy/oauth2_proxy.cfg",
            "--http-address=0.0.0.0:4180",
            "--upstream=file:///dev/null"
          ]

          port {
            container_port = 4180
            protocol       = "TCP"
          }

          env {
            name = "OAUTH2_PROXY_CLIENT_ID"
            value_from {
              secret_key_ref {
                name = kubernetes_secret.oauth2_proxy.metadata[0].name
                key  = "client_id"
              }
            }
          }

          env {
            name = "OAUTH2_PROXY_CLIENT_SECRET"
            value_from {
              secret_key_ref {
                name = kubernetes_secret.oauth2_proxy.metadata[0].name
                key  = "client_secret"
              }
            }
          }

          env {
            name = "OAUTH2_PROXY_COOKIE_SECRET"
            value_from {
              secret_key_ref {
                name = kubernetes_secret.oauth2_proxy.metadata[0].name
                key  = "cookie_secret"
              }
            }
          }

          volume_mount {
            name       = "config"
            mount_path = "/etc/oauth2_proxy"
          }

          resources {
            requests = {
              cpu    = "50m"
              memory = "64Mi"
            }
            limits = {
              cpu    = "100m"
              memory = "128Mi"
            }
          }
        }

        volume {
          name = "config"
          config_map {
            name = kubernetes_config_map.oauth2_proxy.metadata[0].name
          }
        }
      }
    }
  }
}

# Service
resource "kubernetes_service" "oauth2_proxy" {
  metadata {
    name      = "oauth2-proxy"
    namespace = kubernetes_namespace.oauth2_proxy.metadata[0].name
  }

  spec {
    selector = {
      app = "oauth2-proxy"
    }

    port {
      name        = "http"
      port        = 4180
      target_port = 4180
      protocol    = "TCP"
    }

    type = "ClusterIP"
  }
}
```
</Accordion>

<Accordion title="route53.tf - DNS y SSL">
```hcl
# Hosted Zone
resource "aws_route53_zone" "main" {
  name = var.domain_name

  tags = {
    Name = var.domain_name
  }
}

# Certificado ACM
resource "aws_acm_certificate" "main" {
  domain_name               = var.domain_name
  subject_alternative_names = ["*.${var.domain_name}"]
  validation_method         = "DNS"

  lifecycle {
    create_before_destroy = true
  }

  tags = {
    Name = var.domain_name
  }
}

# Validación DNS del certificado
resource "aws_route53_record" "cert_validation" {
  for_each = {
    for dvo in aws_acm_certificate.main.domain_validation_options : dvo.domain_name => {
      name   = dvo.resource_record_name
      record = dvo.resource_record_value
      type   = dvo.resource_record_type
    }
  }

  allow_overwrite = true
  name            = each.value.name
  records         = [each.value.record]
  ttl             = 60
  type            = each.value.type
  zone_id         = aws_route53_zone.main.zone_id
}

resource "aws_acm_certificate_validation" "main" {
  certificate_arn         = aws_acm_certificate.main.arn
  validation_record_fqdns = [for record in aws_route53_record.cert_validation : record.fqdn]
}

# A Record principal
resource "aws_route53_record" "main" {
  zone_id = aws_route53_zone.main.zone_id
  name    = var.domain_name
  type    = "A"

  alias {
    name                   = data.kubernetes_service.ingress_nginx.status[0].load_balancer[0].ingress[0].hostname
    zone_id                = data.aws_elb.ingress_nginx.zone_id
    evaluate_target_health = true
  }
}

# Wildcard A Record
resource "aws_route53_record" "wildcard" {
  zone_id = aws_route53_zone.main.zone_id
  name    = "*.${var.domain_name}"
  type    = "A"

  alias {
    name                   = data.kubernetes_service.ingress_nginx.status[0].load_balancer[0].ingress[0].hostname
    zone_id                = data.aws_elb.ingress_nginx.zone_id
    evaluate_target_health = true
  }
}

# Data sources
data "kubernetes_service" "ingress_nginx" {
  metadata {
    name      = "ingress-nginx-controller"
    namespace = "ingress-nginx"
  }

  depends_on = [helm_release.nginx_ingress]
}

data "aws_elb" "ingress_nginx" {
  name = split("-", split(".", data.kubernetes_service.ingress_nginx.status[0].load_balancer[0].ingress[0].hostname)[0])[0]
}
```
</Accordion>

<Accordion title="variables.tf - Variables">
```hcl
variable "aws_region" {
  description = "AWS region"
  type        = string
  default     = "us-east-1"
}

variable "environment" {
  description = "Environment name"
  type        = string
  default     = "production"
}

variable "cluster_name" {
  description = "EKS cluster name"
  type        = string
  default     = "retrogame-cluster"
}

variable "kubernetes_version" {
  description = "Kubernetes version"
  type        = string
  default     = "1.28"
}

variable "vpc_cidr" {
  description = "VPC CIDR block"
  type        = string
  default     = "10.0.0.0/16"
}

variable "private_subnet_cidrs" {
  description = "Private subnet CIDR blocks"
  type        = list(string)
  default     = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
}

variable "public_subnet_cidrs" {
  description = "Public subnet CIDR blocks"
  type        = list(string)
  default     = ["10.0.101.0/24", "10.0.102.0/24", "10.0.103.0/24"]
}

variable "node_groups" {
  description = "EKS node groups configuration"
  type = map(object({
    instance_types = list(string)
    capacity_type  = string
    min_size       = number
    max_size       = number
    desired_size   = number
    disk_size      = number
    labels         = map(string)
    taints         = list(object({
      key    = string
      value  = string
      effect = string
    }))
    tags = map(string)
  }))
  default = {
    general = {
      instance_types = ["t3.medium"]
      capacity_type  = "ON_DEMAND"
      min_size       = 2
      max_size       = 5
      desired_size   = 3
      disk_size      = 50
      labels         = { role = "general" }
      taints         = []
      tags           = { NodeGroup = "general" }
    }
  }
}

variable "domain_name" {
  description = "Domain name for the application"
  type        = string
  default     = "retrogamehub.games"
}

variable "project_name" {
  description = "Project name for resource naming"
  type        = string
  default     = "retrogame"
}

variable "github_org" {
  description = "GitHub organization for OAuth2"
  type        = string
  default     = "retrogamecloud"
}

variable "github_oauth_client_id" {
  description = "GitHub OAuth App Client ID"
  type        = string
  sensitive   = true
}

variable "github_oauth_client_secret" {
  description = "GitHub OAuth App Client Secret"
  type        = string
  sensitive   = true
}

variable "grafana_admin_password" {
  description = "Grafana admin password"
  type        = string
  sensitive   = true
}
```
</Accordion>

<Accordion title="outputs.tf - Outputs">
```hcl
output "cluster_endpoint" {
  description = "EKS cluster endpoint"
  value       = module.eks.cluster_endpoint
}

output "cluster_name" {
  description = "EKS cluster name"
  value       = module.eks.cluster_name
}

output "cluster_security_group_id" {
  description = "Security group ID attached to the EKS cluster"
  value       = module.eks.cluster_security_group_id
}

output "vpc_id" {
  description = "VPC ID"
  value       = module.vpc.vpc_id
}

output "private_subnets" {
  description = "Private subnet IDs"
  value       = module.vpc.private_subnets
}

output "public_subnets" {
  description = "Public subnet IDs"
  value       = module.vpc.public_subnets
}

output "cdn_url" {
  description = "CloudFront distribution URL"
  value       = "https://${aws_cloudfront_distribution.games_cdn.domain_name}"
}

output "cdn_bucket" {
  description = "S3 bucket for CDN"
  value       = aws_s3_bucket.games_cdn.bucket
}

output "route53_nameservers" {
  description = "Route53 nameservers"
  value       = aws_route53_zone.main.name_servers
}

output "acm_certificate_arn" {
  description = "ACM certificate ARN"
  value       = aws_acm_certificate.main.arn
}

output "grafana_url" {
  description = "Grafana URL"
  value       = "https://grafana.${var.domain_name}"
}

output "prometheus_url" {
  description = "Prometheus URL"
  value       = "https://prometheus.${var.domain_name}"
}

output "alertmanager_url" {
  description = "Alertmanager URL"
  value       = "https://alertmanager.${var.domain_name}"
}
```
</Accordion>

## Comandos Terraform

### Inicialización

```bash
cd infrastructure/terraform/eks

# Inicializar Terraform
terraform init

# Validar configuración
terraform validate

# Formatear código
terraform fmt -recursive
```

### Planificación

```bash
# Ver plan de cambios
terraform plan

# Guardar plan
terraform plan -out=tfplan

# Ver plan guardado
terraform show tfplan
```

### Aplicación

```bash
# Aplicar cambios
terraform apply

# Aplicar plan guardado
terraform apply tfplan

# Auto-aprobar (CI/CD)
terraform apply -auto-approve

# Aplicar objetivo específico
terraform apply -target=module.eks
```

### Destrucción

```bash
# Destruir todo
terraform destroy

# Destruir recurso específico
terraform destroy -target=aws_s3_bucket.games_cdn
```

### Gestión de Estado

```bash
# Listar recursos en estado
terraform state list

# Ver detalle de recurso
terraform state show aws_eks_cluster.main

# Mover recurso en estado
terraform state mv aws_instance.old aws_instance.new

# Eliminar recurso del estado
terraform state rm aws_instance.old

# Pull estado remoto
terraform state pull > terraform.tfstate.backup

# Push estado remoto
terraform state push terraform.tfstate
```

### Workspace

```bash
# Listar workspaces
terraform workspace list

# Crear workspace
terraform workspace new production

# Seleccionar workspace
terraform workspace select production

# Ver workspace actual
terraform workspace show
```

## Variables de Entorno

### terraform.tfvars

```hcl
# terraform.tfvars (NO commitear)
aws_region     = "us-east-1"
environment    = "production"
cluster_name   = "retrogame-cluster"
domain_name    = "retrogamehub.games"

# Secrets
github_oauth_client_id     = "Ov23abcdef1234567890"
github_oauth_client_secret = "1234567890abcdef..."
grafana_admin_password     = "supersecretpassword"

# Node groups
node_groups = {
  general = {
    instance_types = ["t3.medium"]
    capacity_type  = "ON_DEMAND"
    min_size       = 2
    max_size       = 5
    desired_size   = 3
    disk_size      = 50
    labels         = { role = "general" }
    taints         = []
    tags           = { NodeGroup = "general" }
  }
}
```

### Variables desde CLI

```bash
# Pasar variables por línea de comandos
terraform apply \
  -var="cluster_name=retrogame-prod" \
  -var="environment=production"

# Desde archivo
terraform apply -var-file="production.tfvars"

# Desde variables de entorno
export TF_VAR_github_oauth_client_id="Ov23..."
terraform apply
```

## Backend S3

### Configuración

```hcl
terraform {
  backend "s3" {
    bucket         = "retrogame-terraform-state"
    key            = "eks/terraform.tfstate"
    region         = "us-east-1"
    encrypt        = true
    dynamodb_table = "terraform-lock"
  }
}
```

### Crear recursos backend

```bash
# Crear bucket S3
aws s3api create-bucket \
  --bucket retrogame-terraform-state \
  --region us-east-1

# Habilitar versionado
aws s3api put-bucket-versioning \
  --bucket retrogame-terraform-state \
  --versioning-configuration Status=Enabled

# Habilitar encriptación
aws s3api put-bucket-encryption \
  --bucket retrogame-terraform-state \
  --server-side-encryption-configuration '{
    "Rules": [{
      "ApplyServerSideEncryptionByDefault": {
        "SSEAlgorithm": "AES256"
      }
    }]
  }'

# Crear tabla DynamoDB para locks
aws dynamodb create-table \
  --table-name terraform-lock \
  --attribute-definitions AttributeName=LockID,AttributeType=S \
  --key-schema AttributeName=LockID,KeyType=HASH \
  --billing-mode PAY_PER_REQUEST \
  --region us-east-1
```

## CI/CD con GitHub Actions

```yaml
# .github/workflows/terraform.yml
name: Terraform Infrastructure

on:
  push:
    branches: [main]
    paths:
      - 'infrastructure/terraform/**'
  pull_request:
    branches: [main]

env:
  AWS_REGION: us-east-1
  TF_VERSION: 1.6.0

jobs:
  terraform:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
      
      - name: Terraform Format
        run: terraform fmt -check -recursive
        working-directory: infrastructure/terraform/eks
      
      - name: Terraform Init
        run: terraform init
        working-directory: infrastructure/terraform/eks
      
      - name: Terraform Validate
        run: terraform validate
        working-directory: infrastructure/terraform/eks
      
      - name: Terraform Plan
        run: terraform plan -out=tfplan
        working-directory: infrastructure/terraform/eks
        env:
          TF_VAR_github_oauth_client_id: ${{ secrets.GITHUB_OAUTH_CLIENT_ID }}
          TF_VAR_github_oauth_client_secret: ${{ secrets.GITHUB_OAUTH_CLIENT_SECRET }}
          TF_VAR_grafana_admin_password: ${{ secrets.GRAFANA_ADMIN_PASSWORD }}
      
      - name: Terraform Apply
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: terraform apply -auto-approve tfplan
        working-directory: infrastructure/terraform/eks
```

## Costos Estimados

| Recurso | Tipo | Cantidad | Costo Mensual |
|---------|------|----------|---------------|
| EKS Cluster | Control Plane | 1 | $72.00 |
| EC2 Nodes | t3.medium | 3 | $93.60 |
| RDS PostgreSQL | db.t3.micro | 1 | $16.73 |
| ElastiCache Redis | cache.t3.micro | 1 | $13.14 |
| CloudFront | Distribución | 1 | ~$12.43 |
| Route53 | Hosted Zone | 1 | $0.50 |
| ACM Certificate | SSL | 2 | $0.00 |
| ALB | Load Balancer | 1 | $16.20 |
| NAT Gateway | 1 AZ | 1 | $32.40 |
| **TOTAL** | | | **~$256.00/mes** |

## Documentación Relacionada

<CardGroup cols={2}>
  <Card title="EKS Cluster" icon="dharmachakra" href="/infrastructure/eks-cluster">
    Configuración del clúster Kubernetes
  </Card>
  <Card title="CDN CloudFront" icon="cloud" href="/infrastructure/cdn-cloudfront">
    Distribución de contenido
  </Card>
  <Card title="Route53 DNS" icon="globe" href="/infrastructure/route53-ssl">
    DNS y certificados SSL
  </Card>
  <Card title="Monitoring" icon="chart-line" href="/infrastructure/monitoring">
    Prometheus y Grafana
  </Card>
</CardGroup>
