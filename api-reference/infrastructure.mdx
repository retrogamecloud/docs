---
title: "infrastructure"
description: "Documentaci칩n generada autom치ticamente"
icon: "code"
---

<Info>
  Documentaci칩n generada autom치ticamente.
  칔ltima actualizaci칩n: 2025-11-22
</Info>

## Infrastructure: bootstrap
resource "aws_iam_group" "admins" {
  name = "Administrators"
}

--
resource "aws_iam_group_policy_attachment" "admin_policy" {
  group      = aws_iam_group.admins.name
  policy_arn = "arn:aws:iam::aws:policy/AdministratorAccess"
}
--
resource "aws_iam_user" "terraform_user" {
  name          = "retrogamecloud-terraform"
  force_destroy = false

--
resource "aws_iam_user_group_membership" "terraform_membership" {
  user   = aws_iam_user.terraform_user.name
  groups = [aws_iam_group.admins.name]
}
--
resource "aws_iam_user_policy_attachment" "terraform_admin_policy" {
  user       = aws_iam_user.terraform_user.name
  policy_arn = "arn:aws:iam::aws:policy/AdministratorAccess"
}
--
resource "aws_iam_user" "users" {
  for_each      = toset(local.other_admin_users)
  name          = each.key
  force_destroy = false
--
resource "aws_iam_user_group_membership" "admin_membership" {
  for_each = toset(local.other_admin_users)
  user     = aws_iam_user.users[each.key].name
  groups   = [aws_iam_group.admins.name]
--
resource "aws_iam_user_policy_attachment" "user_admin_policy" {
  for_each   = toset(local.other_admin_users)
  user       = aws_iam_user.users[each.key].name
  policy_arn = "arn:aws:iam::aws:policy/AdministratorAccess"
--
resource "aws_iam_access_key" "keys" {
  for_each = toset(local.other_admin_users)
  user     = aws_iam_user.users[each.key].name
}
--
resource "aws_iam_user_login_profile" "user_passwords" {
  for_each                = toset(local.other_admin_users)
  user                    = aws_iam_user.users[each.key].name
  password_length         = 32
## Infrastructure: bootstrap
variable "project_name" {
  description = "Nombre del proyecto"
  type        = string
  default     = "RetroGameCloud"
--
variable "aws_region" {
  description = "Regi칩n de AWS"
  type        = string
  default     = "eu-west-1"
--
variable "aws_profile" {
  description = "Perfil de AWS a usar"
  type        = string
  default     = "retrogamecloud-terraform"
--
variable "environment" {
  description = "Entorno (dev, staging, prod)"
  type        = string
  default     = "shared"
--
variable "admin_users" {
  description = "Lista de usuarios IAM con acceso al bucket de Terraform"
  type        = list(string)
  default     = ["evaristogz", "naesman1", "jpalenz77"]
## Infrastructure: bootstrap
resource "aws_s3_bucket" "terraform_state" {
  bucket        = "retrogamecloud-terraform-state-${data.aws_caller_identity.current.account_id}"
  force_destroy = false

--
resource "aws_s3_bucket_versioning" "terraform_state" {
  bucket = aws_s3_bucket.terraform_state.id

  versioning_configuration {
--
resource "aws_s3_bucket_server_side_encryption_configuration" "terraform_state" {
  bucket = aws_s3_bucket.terraform_state.id

  rule {
--
resource "aws_s3_bucket_public_access_block" "terraform_state" {
  bucket = aws_s3_bucket.terraform_state.id

  block_public_acls       = true
--
resource "aws_s3_bucket_policy" "terraform_state" {
  bucket = aws_s3_bucket.terraform_state.id

  policy = jsonencode({
## Infrastructure: bootstrap
## Infrastructure: bootstrap
## Infrastructure: bootstrap
resource "aws_dynamodb_table" "terraform_lock" {
  name         = "terraform-lock"
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "LockID"
--
resource "aws_iam_policy" "dynamodb_terraform_lock" {
  name        = "TerraformDynamoDBLockAccess"
  description = "Permite a los usuarios de Terraform usar la tabla de locks"

--
resource "aws_iam_user_policy_attachment" "dynamodb_lock_access" {
  for_each = toset(var.admin_users)

  user       = each.key
## Infrastructure: bootstrap
output "terraform_state_bucket" {
  description = "Nombre del bucket S3 para el estado de Terraform"
  value       = aws_s3_bucket.terraform_state.id
}
--
output "terraform_lock_table" {
  description = "Nombre de la tabla DynamoDB para locks"
  value       = aws_dynamodb_table.terraform_lock.id
}
--
output "aws_region" {
  description = "Regi칩n de AWS donde se crearon los recursos"
  value       = var.aws_region
}
--
# output "iam_users_created" {
#   description = "Lista de usuarios IAM creados"
#   value       = keys(aws_iam_user.users)
# }
--
# output "user_passwords" {
#   description = "Contrase침as generadas para los usuarios (solo visible con terraform output -json)"
#   value = {
#     for user in var.admin_users : user => aws_iam_user_login_profile.user_passwords[user].password
#   }
--
# output "user_access_keys" {
#   description = "Access Keys de los usuarios (solo visible con terraform output -json)"
#   value = {
#     for user in var.admin_users : user => {
#       access_key_id     = aws_iam_access_key.keys[user].id
--
output "next_steps" {
  description = "Instrucciones para usar este backend en otros proyectos Terraform"
  value       = <<-EOT
  
--
     terraform output -json user_passwords
     terraform output -json user_access_keys
  
  游닇 Para usar este backend en tus proyectos Terraform (ej: eks_test):
  
## Infrastructure: eks
resource "helm_release" "ingress_nginx" {
  name             = "ingress-nginx"
  repository       = "https://kubernetes.github.io/ingress-nginx"
  chart            = "ingress-nginx"
--
        resources = {
          requests = {
            cpu    = "100m"
            memory = "128Mi"
--
resource "kubernetes_service" "ingress_nginx_alb" {
  metadata {
    name      = "ingress-nginx-alb"
    namespace = "ingress-nginx"
--
output "ingress_nginx_service" {
  description = "Nombre del servicio Ingress NGINX"
  value       = kubernetes_service.ingress_nginx_alb.metadata[0].name
}
## Infrastructure: eks
resource "aws_security_group" "alb" {
  name_prefix = "retrogame-alb-"
  description = "Security group for Application Load Balancer"
  vpc_id      = module.vpc.vpc_id
--
resource "aws_lb" "main" {
  name               = "retrogame-alb"
  internal           = false
  load_balancer_type = "application"
--
resource "aws_lb_target_group" "oauth2_proxy" {
  name        = "retrogame-oauth2-tg"
  port        = 4180
  protocol    = "HTTP"
--
resource "aws_lb_target_group" "frontend" {
  name        = "retrogame-frontend-v2-tg"
  port        = 8081
  protocol    = "HTTP"
--
resource "aws_lb_target_group" "grafana" {
  name        = "retrogame-grafana-tg"
  port        = 3000
  protocol    = "HTTP"
--
resource "aws_lb_target_group" "prometheus" {
  name        = "retrogame-prometheus-tg"
  port        = 9090
  protocol    = "HTTP"
--
resource "aws_lb_target_group" "alertmanager" {
  name        = "retrogame-alertmanager-tg"
  port        = 9093
  protocol    = "HTTP"
--
resource "aws_lb_target_group" "ingress_nginx" {
  name        = "retrogame-ingress-nginx-tg"
  port        = 80
  protocol    = "HTTP"
--
resource "aws_lb_listener" "http" {
  load_balancer_arn = aws_lb.main.arn
  port              = 80
  protocol          = "HTTP"
--
resource "aws_lb_listener" "https" {
  load_balancer_arn = aws_lb.main.arn
  port              = 443
  protocol          = "HTTPS"
--
resource "aws_lb_listener_rule" "monitoring_ingress" {
  listener_arn = aws_lb_listener.https.arn
  priority     = 10

--
resource "aws_lb_listener_rule" "monitoring_ingress_redirect" {
  listener_arn = aws_lb_listener.https.arn
  priority     = 9

--
# Los attachments se har치n con un null_resource que obtenga los IPs de los pods
# despu칠s de que los servicios est칠n corriendo

resource "null_resource" "register_targets" {
  triggers = {
    always_run = timestamp()
  }
## Infrastructure: eks
resource "kubernetes_namespace" "retrogame" {
  metadata {
    name = "retrogame"
  }
--
resource "kubernetes_secret" "jwt_secret" {
  metadata {
    name      = "jwt-secret"
    namespace = kubernetes_namespace.retrogame.metadata[0].name
--
resource "kubernetes_deployment" "backend" {
  metadata {
    name      = "backend"
    namespace = kubernetes_namespace.retrogame.metadata[0].name
--
          resources {
            requests = {
              cpu    = "100m" # Optimizado para t3.micro
              memory = "256Mi"
--
resource "kubernetes_service" "backend" {
  metadata {
    name      = "backend-service"
    namespace = kubernetes_namespace.retrogame.metadata[0].name
--
resource "kubernetes_config_map" "frontend_replacer" {
  metadata {
    name      = "frontend-replacer"
    namespace = kubernetes_namespace.retrogame.metadata[0].name
--
resource "kubernetes_deployment" "frontend" {
  metadata {
    name      = "frontend"
    namespace = kubernetes_namespace.retrogame.metadata[0].name
--
          resources {
            requests = {
              cpu    = "50m"
              memory = "128Mi"
--
resource "kubernetes_service" "frontend" {
  metadata {
    name      = "frontend-service"
    namespace = kubernetes_namespace.retrogame.metadata[0].name
--
resource "kubernetes_config_map" "kong" {
  metadata {
    name      = "kong-declarative-config"
    namespace = kubernetes_namespace.retrogame.metadata[0].name
--
resource "kubernetes_deployment" "kong" {
  wait_for_rollout = false # Temporalmente desactivado para permitir escalado de nodos

  metadata {
--
          resources {
            requests = {
              cpu    = "50m" # Reducido para t3.micro
              memory = "128Mi"
--
resource "kubernetes_service" "kong" {
  metadata {
    name      = "kong-service"
    namespace = kubernetes_namespace.retrogame.metadata[0].name
--
resource "kubernetes_ingress_v1" "backend" {
  metadata {
    name      = "backend-ingress"
    namespace = kubernetes_namespace.retrogame.metadata[0].name
--
resource "kubernetes_ingress_v1" "wiki" {
  metadata {
    name      = "wiki-ingress"
    namespace = kubernetes_namespace.retrogame.metadata[0].name
--
resource "kubernetes_ingress_v1" "wiki_assets" {
  metadata {
    name      = "wiki-assets-ingress"
    namespace = kubernetes_namespace.retrogame.metadata[0].name
--
resource "kubernetes_service" "wiki_external" {
  metadata {
    name      = "wiki-external-service"
    namespace = kubernetes_namespace.retrogame.metadata[0].name
--
resource "kubernetes_ingress_v1" "frontend" {
  metadata {
    name      = "frontend-ingress"
    namespace = kubernetes_namespace.retrogame.metadata[0].name
--
resource "kubernetes_config_map" "db_init_script" {
  metadata {
    name      = "db-init-script"
    namespace = kubernetes_namespace.retrogame.metadata[0].name
--
resource "kubernetes_job" "db_init" {
  metadata {
    name      = "db-init"
    namespace = kubernetes_namespace.retrogame.metadata[0].name
--
# Null resource para verificar las tablas creadas
resource "null_resource" "verify_db_tables" {
  triggers = {
    job_id = kubernetes_job.db_init.metadata[0].uid
  }
--
resource "kubernetes_config_map_v1_data" "frontend_urls" {
  metadata {
    name      = kubernetes_config_map.frontend_replacer.metadata[0].name
    namespace = kubernetes_namespace.retrogame.metadata[0].name
--
# Null resource para aplicar el rollout restart del frontend - COMENTADO: Frontend gestionado por ArgoCD
/* 
resource "null_resource" "restart_frontend" {
  triggers = {
    config_version = kubernetes_config_map_v1_data.frontend_urls.data["replace-urls.sh"]
  }
--
    null_resource.register_targets
  ]
}
*/
## Infrastructure: eks
resource "helm_release" "argocd" {
  name             = "argocd"
  repository       = "https://argoproj.github.io/argo-helm"
  chart            = "argo-cd"
--
resource "kubernetes_ingress_v1" "argocd" {
  metadata {
    name      = "argocd-server"
    namespace = "argocd"
--
output "argocd_admin_password_command" {
  value       = "kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath='{.data.password}' | base64 -d"
  description = "Command to get ArgoCD admin password"
}
## Infrastructure: eks
# SONAR: Logging is configured in aws_s3_bucket_logging.games_cdn resource
# SONAR: HTTPS policy is configured in aws_s3_bucket_policy.games_cdn resource
resource "aws_s3_bucket" "games_cdn" {
  bucket        = "${var.cluster_name}-games-cdn"
  force_destroy = true

--
resource "aws_s3_bucket_public_access_block" "games_cdn" {
  bucket = aws_s3_bucket.games_cdn.id

  block_public_acls       = true
--
# SONAR: HTTPS policy is configured in aws_s3_bucket_policy.cdn_logs resource
# SONAR: ACL log-delivery-write is configured in aws_s3_bucket_acl.cdn_logs resource
resource "aws_s3_bucket" "cdn_logs" {
  bucket        = "${var.cluster_name}-cdn-logs"
  force_destroy = true

--
resource "aws_s3_bucket_ownership_controls" "cdn_logs" {
  bucket = aws_s3_bucket.cdn_logs.id

  rule {
--
resource "aws_s3_bucket_public_access_block" "cdn_logs" {
  bucket = aws_s3_bucket.cdn_logs.id

  block_public_acls       = false
--
resource "aws_s3_bucket_acl" "cdn_logs" {
  bucket = aws_s3_bucket.cdn_logs.id

  access_control_policy {
--
resource "aws_s3_bucket_policy" "cdn_logs" {
  bucket = aws_s3_bucket.cdn_logs.id

  policy = jsonencode({
--
resource "aws_s3_bucket_versioning" "games_cdn" {
  bucket = aws_s3_bucket.games_cdn.id
  versioning_configuration {
    status = "Enabled"
--
resource "aws_s3_bucket_logging" "games_cdn" {
  bucket = aws_s3_bucket.games_cdn.id

  target_bucket = aws_s3_bucket.cdn_logs.id
--
resource "aws_s3_bucket_cors_configuration" "games_cdn" {
  bucket = aws_s3_bucket.games_cdn.id

  cors_rule {
--
resource "aws_s3_bucket_policy" "games_cdn" {
  bucket = aws_s3_bucket.games_cdn.id

  policy = jsonencode({
--
resource "aws_cloudfront_origin_access_control" "games_cdn" {
  name                              = "${var.cluster_name}-games-oac"
  description                       = "OAC for games CDN"
  origin_access_control_origin_type = "s3"
--
resource "aws_cloudfront_distribution" "games_cdn" {
  enabled             = true
  is_ipv6_enabled     = true
  comment             = "CDN for RetroGame static assets"
--
resource "aws_iam_role" "s3_upload_role" {
  name = "${var.cluster_name}-s3-upload-role"

  assume_role_policy = jsonencode({
--
resource "aws_iam_role_policy" "s3_upload_policy" {
  name = "${var.cluster_name}-s3-upload-policy"
  role = aws_iam_role.s3_upload_role.id

--
resource "kubernetes_config_map" "cdn_config" {
  metadata {
    name      = "cdn-config"
    namespace = kubernetes_namespace.retrogame.metadata[0].name
--
resource "null_resource" "upload_static_files" {
  triggers = {
    bucket_id = aws_s3_bucket.games_cdn.id
    # Forzar subida en cada apply
## Infrastructure: eks
variable "aws_region" {
  description = "AWS region para el despliegue"
  type        = string
  default     = "eu-west-1"
--
variable "aws_profile" {
  description = "Perfil de AWS CLI a utilizar"
  type        = string
  default     = "retrogamecloud-terraform"
--
variable "project_name" {
  description = "Nombre del proyecto"
  type        = string
  default     = "RetroGameCloud"
--
variable "environment" {
  description = "Entorno de despliegue (dev, staging, prod)"
  type        = string
  default     = "dev"
--
variable "cluster_name" {
  description = "Nombre del cluster EKS"
  type        = string
  default     = "retrogame"
--
variable "cluster_version" {
  description = "Versi칩n de Kubernetes para EKS"
  type        = string
  default     = "1.34"
--
variable "vpc_cidr" {
  description = "CIDR block para la VPC"
  type        = string
  default     = "10.0.0.0/16"
--
variable "azs" {
  description = "Availability Zones"
  type        = list(string)
  default     = ["eu-west-1a", "eu-west-1b", "eu-west-1c"]
--
variable "private_subnets" {
  description = "CIDR blocks para subnets privadas"
  type        = list(string)
  default     = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
--
variable "public_subnets" {
  description = "CIDR blocks para subnets p칰blicas"
  type        = list(string)
  default     = ["10.0.101.0/24", "10.0.102.0/24", "10.0.103.0/24"]
--
variable "node_instance_types" {
  description = "Tipos de instancia EC2 para los nodos"
  type        = list(string)
  default     = ["t3.small"]
--
variable "node_desired_size" {
  description = "N칰mero deseado de nodos"
  type        = number
  default     = 4
--
variable "node_min_size" {
  description = "N칰mero m칤nimo de nodos"
  type        = number
  default     = 2
--
variable "node_max_size" {
  description = "N칰mero m치ximo de nodos"
  type        = number
  default     = 6
--
variable "enable_cluster_autoscaler" {
  description = "Habilitar Cluster Autoscaler"
  type        = bool
  default     = true
--
variable "enable_metrics_server" {
  description = "Habilitar Metrics Server"
  type        = bool
  default     = true
--
variable "enable_aws_load_balancer_controller" {
  description = "Habilitar AWS Load Balancer Controller"
  type        = bool
  default     = true
--
variable "db_instance_class" {
  description = "Clase de instancia para RDS PostgreSQL"
  type        = string
  default     = "db.t3.micro"
--
variable "db_allocated_storage" {
  description = "Storage asignado para RDS (GB)"
  type        = number
  default     = 20
--
variable "db_name" {
  description = "Nombre de la base de datos"
  type        = string
  default     = "retrogamedb"
--
variable "db_username" {
  description = "Usuario de la base de datos"
  type        = string
  default     = "retrogame"
--
variable "db_password" {
  description = "Contrase침a de la base de datos"
  type        = string
  sensitive   = true
--
variable "jwt_secret" {
  description = "Secret para JWT"
  type        = string
  sensitive   = true
--
variable "tags" {
  description = "Tags adicionales para recursos"
  type        = map(string)
  default     = {}
--
variable "slack_webhook_url" {
  description = "Webhook URL de Slack para alertas de AlertManager"
  type        = string
  sensitive   = true
--
variable "github_oauth_client_id" {
  description = "Client ID de GitHub OAuth App"
  type        = string
  sensitive   = true
--
variable "github_oauth_client_secret" {
  description = "Client Secret de GitHub OAuth App"
  type        = string
  sensitive   = true
## Infrastructure: eks
resource "kubernetes_ingress_v1" "grafana" {
  metadata {
    name      = "grafana-ingress"
    namespace = "monitoring"
--
resource "kubernetes_ingress_v1" "prometheus" {
  metadata {
    name      = "prometheus-ingress"
    namespace = "monitoring"
--
resource "kubernetes_ingress_v1" "alertmanager" {
  metadata {
    name      = "alertmanager-ingress"
    namespace = "monitoring"
--
resource "kubernetes_ingress_v1" "oauth2_proxy" {
  metadata {
    name      = "oauth2-proxy-ingress"
    namespace = "monitoring"
## Infrastructure: eks
      variable = "${replace(module.eks.cluster_oidc_issuer_url, "https://", "")}:sub"
      values   = ["system:serviceaccount:kube-system:ebs-csi-controller-sa"]
    }

--
      variable = "${replace(module.eks.cluster_oidc_issuer_url, "https://", "")}:aud"
      values   = ["sts.amazonaws.com"]
    }
  }
--
resource "aws_iam_role" "ebs_csi_driver" {
  name               = "${var.cluster_name}-ebs-csi-driver"
  assume_role_policy = data.aws_iam_policy_document.ebs_csi_driver_assume_role.json

--
resource "aws_iam_role_policy_attachment" "ebs_csi_driver" {
  role       = aws_iam_role.ebs_csi_driver.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy"
}
--
resource "aws_eks_addon" "ebs_csi_driver" {
  cluster_name             = module.eks.cluster_name
  addon_name               = "aws-ebs-csi-driver"
  addon_version            = "v1.37.0-eksbuild.1"  # Compatible con K8s 1.34
## Infrastructure: eks
resource "random_password" "oauth2_proxy_cookie_secret" {
  length  = 32
  special = true
}
--
resource "kubernetes_secret" "oauth2_proxy" {
  metadata {
    name      = "oauth2-proxy"
    namespace = "monitoring"
--
resource "kubernetes_config_map" "oauth2_proxy" {
  metadata {
    name      = "oauth2-proxy"
    namespace = "monitoring"
--
resource "kubernetes_deployment" "oauth2_proxy" {
  metadata {
    name      = "oauth2-proxy"
    namespace = "monitoring"
--
          resources {
            requests = {
              cpu    = "50m"
              memory = "64Mi"
--
resource "kubernetes_service" "oauth2_proxy" {
  metadata {
    name      = "oauth2-proxy"
    namespace = "monitoring"
--
output "oauth2_proxy_service_name" {
  description = "Nombre del servicio oauth2-proxy"
  value       = kubernetes_service.oauth2_proxy.metadata[0].name
}
## Infrastructure: eks
## Infrastructure: eks
resource "aws_security_group" "rds" {
  name_prefix = "${var.cluster_name}-rds-sg"
  vpc_id      = module.vpc.vpc_id

--
resource "aws_db_subnet_group" "postgres" {
  name       = "${var.cluster_name}-db-subnet"
  subnet_ids = module.vpc.private_subnets

--
resource "aws_db_instance" "postgres" {
  identifier     = "${var.cluster_name}-postgres"
  engine         = "postgres"
  engine_version = "15.15"
--
resource "kubernetes_secret" "postgres_credentials" {
  metadata {
    name      = "postgres-credentials"
    namespace = kubernetes_namespace.retrogame.metadata[0].name
--
resource "kubernetes_config_map" "postgres_init" {
  metadata {
    name      = "postgres-init-script"
    namespace = kubernetes_namespace.retrogame.metadata[0].name
## Infrastructure: eks
resource "kubernetes_namespace" "monitoring" {
  metadata {
    name = "monitoring"
    labels = {
--
resource "kubernetes_secret" "alertmanager_slack" {
  metadata {
    name      = "alertmanager-slack-webhook"
    namespace = kubernetes_namespace.monitoring.metadata[0].name
--
resource "helm_release" "kube_prometheus_stack" {
  name       = "kube-prometheus-stack"
  repository = "https://prometheus-community.github.io/helm-charts"
  chart      = "kube-prometheus-stack"
--
          resources = {
            requests = {
              cpu    = "200m"
              memory = "512Mi"
--
                resources = {
                  requests = {
                    storage = "10Gi"
                  }
--
        resources = {
          requests = {
            cpu    = "100m"
            memory = "256Mi"
--
            kubernetes-resources = {
              gnetId     = 10000
              revision   = 1
              datasource = "Prometheus"
--
          resources = {
            requests = {
              cpu    = "50m"
              memory = "128Mi"
--
                resources = {
                  requests = {
                    storage = "2Gi"
                  }
--
        resources = {
          requests = {
            cpu    = "50m"
            memory = "64Mi"
--
        resources = {
          requests = {
            cpu    = "50m"
            memory = "128Mi"
--
        resources = {
          requests = {
            cpu    = "100m"
            memory = "128Mi"
--
# Outputs movidos a outputs.tf para evitar duplicados
# ============================================================================
## Infrastructure: eks
resource "aws_route53_zone" "main" {
  name = "retrogamehub.games"

  tags = {
--
resource "aws_acm_certificate" "main" {
  domain_name       = "retrogamehub.games"
  validation_method = "DNS"

--
resource "aws_route53_record" "cert_validation" {
  for_each = {
    for dvo in aws_acm_certificate.main.domain_validation_options : dvo.domain_name => {
      name   = dvo.resource_record_name
      record = dvo.resource_record_value
      type   = dvo.resource_record_type
    }
  }

--
resource "aws_acm_certificate_validation" "main" {
  certificate_arn         = aws_acm_certificate.main.arn
  validation_record_fqdns = [for record in aws_route53_record.cert_validation : record.fqdn]

--
resource "aws_route53_record" "main" {
  zone_id = aws_route53_zone.main.zone_id
  name    = "retrogamehub.games"
  type    = "A"
--
resource "aws_route53_record" "wildcard" {
  zone_id = aws_route53_zone.main.zone_id
  name    = "*.retrogamehub.games"
  type    = "A"
## Infrastructure: eks
# Lee los outputs del proyecto bootstrap para obtener bucket y tabla DynamoDB

data "terraform_remote_state" "bootstrap" {
  backend = "local"
## Infrastructure: eks
resource "aws_security_group" "node_group" {
  name_prefix = "${var.cluster_name}-node-sg"
  vpc_id      = module.vpc.vpc_id

--
resource "aws_iam_role" "eks_admin" {
  name = "${var.cluster_name}-admin-role"

  assume_role_policy = jsonencode({
--
resource "null_resource" "update_kubeconfig" {
  depends_on = [module.eks]

  triggers = {
--
resource "null_resource" "force_node_update" {
  depends_on = [module.eks]

  triggers = {
## Infrastructure: eks
output "cluster_name" {
  description = "Nombre del cluster EKS"
  value       = module.eks.cluster_name
}
--
output "cluster_endpoint" {
  description = "Endpoint del cluster EKS"
  value       = module.eks.cluster_endpoint
}
--
output "cluster_security_group_id" {
  description = "Security group del cluster EKS"
  value       = module.eks.cluster_security_group_id
}
--
output "cluster_oidc_issuer_url" {
  description = "OIDC issuer URL del cluster"
  value       = module.eks.cluster_oidc_issuer_url
}
--
output "region" {
  description = "AWS region"
  value       = var.aws_region
}
--
output "vpc_id" {
  description = "ID de la VPC"
  value       = module.vpc.vpc_id
}
--
output "vpc_cidr" {
  description = "CIDR block de la VPC"
  value       = module.vpc.vpc_cidr_block
}
--
output "private_subnets" {
  description = "IDs de las subnets privadas"
  value       = module.vpc.private_subnets
}
--
output "public_subnets" {
  description = "IDs de las subnets p칰blicas"
  value       = module.vpc.public_subnets
}
--
output "rds_endpoint" {
  description = "Endpoint de la base de datos RDS PostgreSQL"
  value       = aws_db_instance.postgres.endpoint
}
--
output "rds_database_name" {
  description = "Nombre de la base de datos"
  value       = aws_db_instance.postgres.db_name
}
--
output "kong_load_balancer_hostname" {
  description = "Hostname del Load Balancer de Kong"
  value       = try(kubernetes_service.kong.status[0].load_balancer[0].ingress[0].hostname, "pending")
}
--
output "configure_kubectl" {
  description = "Comando para configurar kubectl"
  value       = "aws eks update-kubeconfig --region ${var.aws_region} --name ${module.eks.cluster_name}"
}
--
output "node_group_security_group_id" {
  description = "Security group de los node groups"
  value       = aws_security_group.node_group.id
}
--
output "rds_security_group_id" {
  description = "Security group de RDS"
  value       = aws_security_group.rds.id
}
--
output "s3_games_bucket" {
  description = "Nombre del bucket S3 para juegos"
  value       = aws_s3_bucket.games_cdn.id
}
--
output "cloudfront_distribution_id" {
  description = "ID de la distribuci칩n CloudFront"
  value       = aws_cloudfront_distribution.games_cdn.id
}
--
output "cloudfront_domain_name" {
  description = "Domain name de CloudFront para acceder a los juegos"
  value       = aws_cloudfront_distribution.games_cdn.domain_name
}
--
output "cdn_url" {
  description = "URL completa del CDN"
  value       = "https://${aws_cloudfront_distribution.games_cdn.domain_name}"
}
--
output "upload_games_command" {
  description = "Comando para subir juegos al bucket S3"
  value       = "aws s3 sync ./infraestructure/cdn/juegos/ s3://${aws_s3_bucket.games_cdn.id}/juegos/ --region ${var.aws_region}"
}
--
output "upload_images_command" {
  description = "Comando para subir im치genes al bucket S3"
  value       = "aws s3 sync ./infraestructure/cdn/img/ s3://${aws_s3_bucket.games_cdn.id}/img/ --region ${var.aws_region}"
}
--
output "route53_zone_id" {
  description = "ID de la zona Route53"
  value       = try(aws_route53_zone.main.zone_id, null)
}
--
output "route53_nameservers" {
  description = "Nameservers de Route53 (configurar en Namecheap)"
  value       = try(aws_route53_zone.main.name_servers, null)
}
--
output "ssl_certificate_arn" {
  description = "ARN del certificado SSL"
  value       = try(aws_acm_certificate.main.arn, null)
}
--
output "alb_dns_name" {
  description = "DNS name del Application Load Balancer"
  value       = try(aws_lb.main.dns_name, null)
}
--
output "alb_zone_id" {
  description = "Zone ID del ALB"
  value       = try(aws_lb.main.zone_id, null)
}
--
output "domain_url" {
  description = "URL del dominio principal"
  value       = "https://retrogamehub.games"
}
--
output "grafana_url" {
  description = "URL de Grafana"
  value       = "https://retrogamehub.games/grafana"
}
--
output "prometheus_url" {
  description = "URL de Prometheus"
  value       = "https://retrogamehub.games/prometheus"
}
--
output "alertmanager_url" {
  description = "URL de AlertManager"
  value       = "https://retrogamehub.games/alertmanager"
}
--
output "grafana_admin_user" {
  description = "Usuario admin de Grafana"
  value       = "admin"
}
--
output "grafana_admin_password" {
  description = "Password admin de Grafana"
  value       = "admin123"
  sensitive   = true
--
output "prometheus_internal_url" {
  description = "URL interna de Prometheus Server"
  value       = "http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090"
}
--
output "grafana_internal_url" {
  description = "URL interna de Grafana"
  value       = "http://kube-prometheus-stack-grafana.monitoring.svc.cluster.local:80"
}
--
output "alertmanager_internal_url" {
  description = "URL interna de AlertManager"
  value       = "http://kube-prometheus-stack-alertmanager.monitoring.svc.cluster.local:9093"
}
--
output "grafana_port_forward_command" {
  description = "Comando para acceder a Grafana localmente"
  value       = "kubectl port-forward -n monitoring svc/kube-prometheus-stack-grafana 3000:80"
}
--
output "prometheus_port_forward_command" {
  description = "Comando para acceder a Prometheus localmente"
  value       = "kubectl port-forward -n monitoring svc/kube-prometheus-stack-prometheus 9090:9090"
}

