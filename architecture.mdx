---
title: Arquitectura del Sistema
sidebarTitle: 1.3. Arquitectura del Sistema
description: "VisiÃ³n general de la arquitectura de microservicios cloud-native de Retro Game Hub desplegada en AWS EKS, incluyendo decisiones arquitectÃ³nicas, infraestructura y justificaciones tÃ©cnicas"
icon: file-lines
---

## VisiÃ³n General de la Arquitectura

Retro Game Hub estÃ¡ construido usando una arquitectura de microservicios cloud-native desplegada en AWS EKS (Kubernetes). La arquitectura sigue principios de escalabilidad, alta disponibilidad y separaciÃ³n de responsabilidades, organizÃ¡ndose en capas claramente definidas que aprovechan servicios gestionados de AWS para optimizar el rendimiento y reducir la complejidad operacional.

## Arquitectura de Alto Nivel

```mermaid
graph TB
    subgraph "INTERNET"
        Users[ğŸ‘¥ Users<br/>Global Traffic]
    end

    subgraph "AWS GLOBAL"
        subgraph "Route 53"
            DNS[ğŸŒ Route 53<br/>DNS Resolution<br/>retrogamehub.com]
        end

        subgraph "CloudFront Global"
            CDN[ğŸ“¡ CloudFront CDN<br/>Edge Locations<br/>Static Assets & Game Files]
        end
    end

    subgraph "AWS REGION: us-east-1"
        subgraph "VPC: 10.0.0.0/16"
            subgraph "Public Subnets"
                subgraph "AZ-1a: 10.0.1.0/24"
                    ALB1[âš–ï¸ Application Load Balancer<br/>SSL/TLS Termination<br/>Target Groups]
                    NAT1[ğŸ”„ NAT Gateway<br/>Outbound Internet]
                end

                subgraph "AZ-1b: 10.0.2.0/24"
                    NAT2[ğŸ”„ NAT Gateway<br/>Outbound Internet]
                end

                subgraph "AZ-1c: 10.0.3.0/24"
                    NAT3[ğŸ”„ NAT Gateway<br/>Outbound Internet]
                end
            end

            subgraph "Private Subnets - Application Tier"
                subgraph "App-AZ-1a: 10.0.10.0/24"
                    EKS1[â˜¸ï¸ EKS Worker Nodes<br/>Application Pods<br/>c5.large instances]
                end

                subgraph "App-AZ-1b: 10.0.20.0/24"
                    EKS2[â˜¸ï¸ EKS Worker Nodes<br/>Application Pods<br/>c5.large instances]
                end

                subgraph "App-AZ-1c: 10.0.30.0/24"
                    EKS3[â˜¸ï¸ EKS Worker Nodes<br/>Application Pods<br/>c5.large instances]
                end
            end

            subgraph "Private Subnets - Data Tier"
                subgraph "Data-AZ-1a: 10.0.100.0/24"
                    RDS1[ğŸ—„ï¸ RDS Aurora PostgreSQL<br/>Primary Instance<br/>db.r5.large]
                    REDIS1[ğŸš€ ElastiCache Redis<br/>Primary Node<br/>cache.r6g.large]
                end

                subgraph "Data-AZ-1b: 10.0.200.0/24"
                    RDS2[ğŸ—„ï¸ RDS Aurora PostgreSQL<br/>Reader Replica<br/>db.r5.large]
                    REDIS2[ğŸš€ ElastiCache Redis<br/>Replica Node<br/>cache.r6g.large]
                end

                subgraph "Data-AZ-1c: 10.0.300.0/24"
                    RDS3[ğŸ—„ï¸ RDS Aurora PostgreSQL<br/>Reader Replica<br/>db.r5.large]
                    REDIS3[ğŸš€ ElastiCache Redis<br/>Replica Node<br/>cache.r6g.large]
                end
            end

            subgraph "EKS Control Plane"
                EKSCP[â˜¸ï¸ EKS Control Plane<br/>Multi-AZ Managed<br/>API Server, etcd, Scheduler]
            end

            subgraph "Supporting Services"
                S3[ğŸª£ S3 Buckets<br/>Game Files Storage<br/>Static Assets]
                SECRETS[ğŸ” AWS Secrets Manager<br/>Database Credentials<br/>API Keys]
                ECR[ğŸ“¦ ECR<br/>Container Registry<br/>Docker Images]
            end
        end
    end

    %% Traffic Flow
    Users --> DNS
    DNS --> CDN
    CDN --> ALB1
    ALB1 --> EKS1
    ALB1 --> EKS2
    ALB1 --> EKS3
    
    %% Data Flow
    EKS1 --> RDS1
    EKS1 --> REDIS1
    EKS2 --> RDS2
    EKS2 --> REDIS2
    EKS3 --> RDS3
    EKS3 --> REDIS3

    %% EKS Management
    EKSCP -.-> EKS1
    EKSCP -.-> EKS2
    EKSCP -.-> EKS3

    %% Outbound Connectivity
    EKS1 --> NAT1
    EKS2 --> NAT2
    EKS3 --> NAT3

    %% Supporting Services
    EKS1 -.-> S3
    EKS1 -.-> SECRETS
    EKS1 -.-> ECR
    
    CDN -.-> S3

    %% Styling
    classDef publicSubnet fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef privateSubnet fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef dataSubnet fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef awsService fill:#ff9800,stroke:#e65100,stroke-width:2px
    classDef eksService fill:#4caf50,stroke:#2e7d32,stroke-width:2px

    class ALB1,NAT1,NAT2,NAT3 publicSubnet
    class EKS1,EKS2,EKS3 privateSubnet
    class RDS1,RDS2,RDS3,REDIS1,REDIS2,REDIS3 dataSubnet
    class S3,SECRETS,ECR,CDN,DNS awsService
    class EKSCP eksService
```

## Flujo de TrÃ¡fico y Conectividad

```mermaid
sequenceDiagram
    participant U as ğŸ‘¥ User
    participant R53 as ğŸŒ Route 53
    participant CF as ğŸ“¡ CloudFront
    participant ALB as âš–ï¸ ALB
    participant EKS as â˜¸ï¸ EKS Pod
    participant RDS as ğŸ—„ï¸ Aurora
    participant REDIS as ğŸš€ Redis
    participant S3 as ğŸª£ S3

    Note over U,S3: Game Loading Flow

    U->>R53: DNS Query (retrogamehub.com)
    R53->>U: CloudFront IP Address
    
    U->>CF: HTTPS Request
    Note over CF: Edge Location Cache Check
    
    alt Static Assets (CSS/JS/Images)
        CF->>S3: Fetch from Origin (if cache miss)
        S3->>CF: Static Content
        CF->>U: Cached Content (Global Edge)
    else Dynamic API Requests
        CF->>ALB: Forward to Origin
        ALB->>EKS: Route to Available Pod
        
        Note over EKS: Microservice Processing
        
        EKS->>REDIS: Check Cache
        alt Cache Hit
            REDIS->>EKS: Return Cached Data
        else Cache Miss
            EKS->>RDS: Database Query
            RDS->>EKS: Query Results
            EKS->>REDIS: Update Cache
        end
        
        EKS->>ALB: API Response
        ALB->>CF: Forward Response
        CF->>U: Dynamic Content
    end

    Note over U,S3: Game File Download Flow
    U->>CF: Request Game ROM/Assets
    CF->>S3: Fetch Game Files (if not cached)
    S3->>CF: Game File Content
    CF->>U: Optimized Delivery
```

## Componentes Principales

### Capa de Red y Conectividad

- **Route 53**: DNS global con health checks y routing policies
- **CloudFront CDN**: 
  - 200+ edge locations globales
  - CachÃ© de assets estÃ¡ticos y archivos de juegos
  - CompresiÃ³n automÃ¡tica y optimizaciÃ³n
  - SSL/TLS termination

### Infraestructura de Compute

- **AWS EKS**: Kubernetes gestionado multi-AZ
  - Worker nodes distribuidos en 3 AZs
  - Auto Scaling Groups para elasticidad
  - Spot instances para cargas no crÃ­ticas
  - **Application Load Balancer**: 
    - SSL termination con AWS Certificate Manager
    - Target groups para microservicios
    - Health checks y routing inteligente

### Capa de Datos

- **Amazon Aurora PostgreSQL**:
  - Cluster multi-AZ con 1 writer y 2+ readers
  - Automatic failover en menos de 30 segundos
  - Continuous backup y point-in-time recovery

- **ElastiCache Redis**:
  - Cluster mode habilitado para sharding
  - Multi-AZ con automatic failover
  - Encryption in-transit y at-rest

### Almacenamiento y Assets

- **Amazon S3**:
  - Bucket para game files con versionado
  - Bucket para static assets con CloudFront integration
  - Intelligent Tiering para optimizaciÃ³n de costos
  - **Amazon ECR**: Registry privado para imÃ¡genes Docker

### Seguridad y Secrets Management

- **AWS Secrets Manager**: RotaciÃ³n automÃ¡tica de credenciales
- **IAM Roles**: Service accounts con OIDC integration
- **Security Groups**: MicrosegmentaciÃ³n de red
- **NACLs**: Capa adicional de seguridad de subnet

## Decisiones ArquitectÃ³nicas Clave

### Multi-AZ para Alta Disponibilidad

Todos los componentes crÃ­ticos estÃ¡n distribuidos en mÃºltiples AZs:
- **RTO objetivo**: < 1 minuto para fallos de AZ
- **RPO objetivo**: < 5 minutos para pÃ©rdida de datos
- **Disponibilidad objetivo**: 99.9% uptime

### SeparaciÃ³n de Capas de Red

```
Public Subnets (10.0.1-3.0/24)     -> Internet-facing resources
Private App Subnets (10.0.10-30.0/24) -> EKS worker nodes  
Private Data Subnets (10.0.100-300.0/24) -> Databases
```

### Estrategia de CachÃ© Multi-Nivel

1. **CloudFront**: Assets estÃ¡ticos y game files
2. **Redis**: Datos de aplicaciÃ³n y sesiones
3. **Aurora Read Replicas**: Query caching y read scaling

### Escalabilidad AutomÃ¡tica

- **Cluster Autoscaler**: Nodos EKS basado en pod scheduling
- **HPA**: Pods basado en CPU/memoria
- **VPA**: Vertical scaling para optimizaciÃ³n de recursos
- **Aurora Auto Scaling**: Read replicas basado en conexiones/CPU

## Justificaciones TÃ©cnicas

### Â¿Por quÃ© EKS sobre ECS/Lambda?

1. **Complejidad de Microservicios**: 8+ servicios interconectados
2. **Portabilidad**: Kubernetes-native, avoid vendor lock-in
3. **Ecosistema**: Helm charts, operators, service mesh ready
4. **Scaling Granular**: Pod-level scaling vs container/function level

### Â¿Por quÃ© Aurora sobre RDS MySQL?

1. **Performance**: 3x throughput vs MySQL
2. **Scaling**: Storage auto-scaling hasta 128TB
3. **Backups**: Continuous backup sin impacto en performance
4. **Global**: Aurora Global Database para DR futuro

### Â¿Por quÃ© Redis sobre DynamoDB para CachÃ©?

1. **Data Structures**: Support para gaming leaderboards, sets
2. **Pub/Sub**: Real-time notifications entre usuarios
3. **Lua Scripts**: Complex operations atomically
4. **Gaming Patterns**: Session storage, temporary data

## Monitoreo y Observabilidad

### MÃ©tricas Clave (CloudWatch)

- **Infrastructure**: CPU, memoria, network, disk de EKS nodes
- **Application**: Request latency, error rates, throughput
- **Database**: Aurora connections, query performance, lag
- **Cache**: Redis hit/miss ratios, memory usage

### Logging Strategy

- **Application Logs**: Fluent Bit -> CloudWatch Logs
- **Access Logs**: ALB -> S3 para analysis
- **Audit Logs**: EKS API calls -> CloudTrail
- **Database Logs**: Aurora slow query logs

### Alerting (CloudWatch Alarms)

- **Critical**: Database failover, EKS node failures
- **Warning**: High CPU/memory, elevated error rates  
- **Info**: Scaling events, deployment status

Esta arquitectura proporciona una base sÃ³lida y escalable para Retro Game Hub, balanceando performance, disponibilidad y costos mientras mantiene la flexibilidad para evolucionar con los requerimientos del negocio.